{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import r2_score, explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from scipy import stats\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from fancyimpute import KNN, SoftImpute\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Impute all variable \n",
    "2. Implement Normalitation to variable 'age' and 'MonthlyIncome'\n",
    "3. Given this case linier or nonlinier\n",
    "4. Traing using algorithm resolve linier case if this case linier or algorithm resolve nonlinier if this case non linier\n",
    "5. Do Hyper Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Train and Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size\n",
      "Training Size : (150000, 11)\n",
      "Test Size : (101503, 11)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = pd.read_csv('cs-training.csv', delimiter = ',').drop('Unnamed: 0', axis = 1)\n",
    "test_dataset = pd.read_csv('cs-test.csv', delimiter = ',').drop('Unnamed: 0', axis = 1)\n",
    "print(\"Dataset Size\")\n",
    "print(\"Training Size : {}\".format(training_dataset.shape))\n",
    "print(\"Test Size : {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.213179</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.375607</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.305682</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>5710.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.754464</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.209940</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.116951</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.189169</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>23684.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0                 1                              0.766127   45   \n",
       "1                 0                              0.957151   40   \n",
       "2                 0                              0.658180   38   \n",
       "3                 0                              0.233810   30   \n",
       "4                 0                              0.907239   49   \n",
       "5                 0                              0.213179   74   \n",
       "6                 0                              0.305682   57   \n",
       "7                 0                              0.754464   39   \n",
       "8                 0                              0.116951   27   \n",
       "9                 0                              0.189169   57   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                     2     0.802982         9120.0   \n",
       "1                                     0     0.121876         2600.0   \n",
       "2                                     1     0.085113         3042.0   \n",
       "3                                     0     0.036050         3300.0   \n",
       "4                                     1     0.024926        63588.0   \n",
       "5                                     0     0.375607         3500.0   \n",
       "6                                     0  5710.000000            NaN   \n",
       "7                                     0     0.209940         3500.0   \n",
       "8                                     0    46.000000            NaN   \n",
       "9                                     0     0.606291        23684.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               13                        0   \n",
       "1                                4                        0   \n",
       "2                                2                        1   \n",
       "3                                5                        0   \n",
       "4                                7                        0   \n",
       "5                                3                        0   \n",
       "6                                8                        0   \n",
       "7                                8                        0   \n",
       "8                                2                        0   \n",
       "9                                9                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             6                                     0   \n",
       "1                             0                                     0   \n",
       "2                             0                                     0   \n",
       "3                             0                                     0   \n",
       "4                             1                                     0   \n",
       "5                             1                                     0   \n",
       "6                             3                                     0   \n",
       "7                             0                                     0   \n",
       "8                             0                                     0   \n",
       "9                             4                                     0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "5                 1.0  \n",
       "6                 0.0  \n",
       "7                 0.0  \n",
       "8                 NaN  \n",
       "9                 2.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885519</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177513</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.463295</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0.527237</td>\n",
       "      <td>9141.0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043275</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687648</td>\n",
       "      <td>5083.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.280308</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925961</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>3865.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509791</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.342429</td>\n",
       "      <td>4140.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.587778</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1048.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046149</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>0.369170</td>\n",
       "      <td>3301.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0               NaN                              0.885519   43   \n",
       "1               NaN                              0.463295   57   \n",
       "2               NaN                              0.043275   59   \n",
       "3               NaN                              0.280308   38   \n",
       "4               NaN                              1.000000   27   \n",
       "5               NaN                              0.509791   63   \n",
       "6               NaN                              0.587778   50   \n",
       "7               NaN                              0.046149   79   \n",
       "8               NaN                              0.013527   68   \n",
       "9               NaN                              1.000000   23   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                     0     0.177513         5700.0   \n",
       "1                                     0     0.527237         9141.0   \n",
       "2                                     0     0.687648         5083.0   \n",
       "3                                     1     0.925961         3200.0   \n",
       "4                                     0     0.019917         3865.0   \n",
       "5                                     0     0.342429         4140.0   \n",
       "6                                     0  1048.000000            0.0   \n",
       "7                                     1     0.369170         3301.0   \n",
       "8                                     0  2024.000000            NaN   \n",
       "9                                    98     0.000000            0.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                                4                        0   \n",
       "1                               15                        0   \n",
       "2                               12                        0   \n",
       "3                                7                        0   \n",
       "4                                4                        0   \n",
       "5                                4                        0   \n",
       "6                                5                        0   \n",
       "7                                8                        0   \n",
       "8                                4                        0   \n",
       "9                                0                       98   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             0                                     0   \n",
       "1                             4                                     0   \n",
       "2                             1                                     0   \n",
       "3                             2                                     0   \n",
       "4                             0                                     0   \n",
       "5                             0                                     0   \n",
       "6                             0                                     0   \n",
       "7                             1                                     0   \n",
       "8                             1                                     0   \n",
       "9                             0                                    98   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 2.0  \n",
       "2                 2.0  \n",
       "3                 0.0  \n",
       "4                 1.0  \n",
       "5                 1.0  \n",
       "6                 3.0  \n",
       "7                 1.0  \n",
       "8                 0.0  \n",
       "9                 0.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149990</th>\n",
       "      <td>0</td>\n",
       "      <td>0.055518</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609779</td>\n",
       "      <td>4335.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149991</th>\n",
       "      <td>0</td>\n",
       "      <td>0.104112</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.477658</td>\n",
       "      <td>10316.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149992</th>\n",
       "      <td>0</td>\n",
       "      <td>0.871976</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>4132.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149993</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>820.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149994</th>\n",
       "      <td>0</td>\n",
       "      <td>0.385742</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.404293</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.040674</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225131</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.299745</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716562</td>\n",
       "      <td>5584.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>0</td>\n",
       "      <td>0.246044</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3870.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5716.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>0</td>\n",
       "      <td>0.850283</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249908</td>\n",
       "      <td>8158.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "149990                 0                              0.055518   46   \n",
       "149991                 0                              0.104112   59   \n",
       "149992                 0                              0.871976   50   \n",
       "149993                 0                              1.000000   22   \n",
       "149994                 0                              0.385742   50   \n",
       "149995                 0                              0.040674   74   \n",
       "149996                 0                              0.299745   44   \n",
       "149997                 0                              0.246044   58   \n",
       "149998                 0                              0.000000   30   \n",
       "149999                 0                              0.850283   64   \n",
       "\n",
       "        NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "149990                                     0     0.609779         4335.0   \n",
       "149991                                     0     0.477658        10316.0   \n",
       "149992                                     0  4132.000000            NaN   \n",
       "149993                                     0     0.000000          820.0   \n",
       "149994                                     0     0.404293         3400.0   \n",
       "149995                                     0     0.225131         2100.0   \n",
       "149996                                     0     0.716562         5584.0   \n",
       "149997                                     0  3870.000000            NaN   \n",
       "149998                                     0     0.000000         5716.0   \n",
       "149999                                     0     0.249908         8158.0   \n",
       "\n",
       "        NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "149990                                7                        0   \n",
       "149991                               10                        0   \n",
       "149992                               11                        0   \n",
       "149993                                1                        0   \n",
       "149994                                7                        0   \n",
       "149995                                4                        0   \n",
       "149996                                4                        0   \n",
       "149997                               18                        0   \n",
       "149998                                4                        0   \n",
       "149999                                8                        0   \n",
       "\n",
       "        NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "149990                             1                                     0   \n",
       "149991                             2                                     0   \n",
       "149992                             1                                     0   \n",
       "149993                             0                                     0   \n",
       "149994                             0                                     0   \n",
       "149995                             1                                     0   \n",
       "149996                             1                                     0   \n",
       "149997                             1                                     0   \n",
       "149998                             0                                     0   \n",
       "149999                             2                                     0   \n",
       "\n",
       "        NumberOfDependents  \n",
       "149990                 2.0  \n",
       "149991                 0.0  \n",
       "149992                 3.0  \n",
       "149993                 0.0  \n",
       "149994                 0.0  \n",
       "149995                 0.0  \n",
       "149996                 2.0  \n",
       "149997                 0.0  \n",
       "149998                 0.0  \n",
       "149999                 0.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101493</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035549</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>11128.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101494</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.218356</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0.295803</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101495</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.718874</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.308047</td>\n",
       "      <td>4125.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021654</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101497</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045230</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.282653</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068522</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922156</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.934217</td>\n",
       "      <td>7615.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081596</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>836.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101501</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.335457</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>3568.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.441842</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198918</td>\n",
       "      <td>5916.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "101493               NaN                              0.035549   58   \n",
       "101494               NaN                              0.218356   56   \n",
       "101495               NaN                              0.718874   35   \n",
       "101496               NaN                              0.021654   78   \n",
       "101497               NaN                              0.045230   67   \n",
       "101498               NaN                              0.282653   24   \n",
       "101499               NaN                              0.922156   36   \n",
       "101500               NaN                              0.081596   70   \n",
       "101501               NaN                              0.335457   56   \n",
       "101502               NaN                              0.441842   29   \n",
       "\n",
       "        NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "101493                                     0     0.290323        11128.0   \n",
       "101494                                     0     0.295803         1500.0   \n",
       "101495                                     1     0.308047         4125.0   \n",
       "101496                                     0    18.000000            NaN   \n",
       "101497                                     0     0.012198         5000.0   \n",
       "101498                                     0     0.068522         1400.0   \n",
       "101499                                     3     0.934217         7615.0   \n",
       "101500                                     0   836.000000            NaN   \n",
       "101501                                     0  3568.000000            NaN   \n",
       "101502                                     0     0.198918         5916.0   \n",
       "\n",
       "        NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "101493                                7                        0   \n",
       "101494                                3                        0   \n",
       "101495                                8                        0   \n",
       "101496                                8                        0   \n",
       "101497                                4                        0   \n",
       "101498                                5                        0   \n",
       "101499                                8                        0   \n",
       "101500                                3                        0   \n",
       "101501                                8                        0   \n",
       "101502                               12                        0   \n",
       "\n",
       "        NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "101493                             2                                     0   \n",
       "101494                             0                                     0   \n",
       "101495                             0                                     1   \n",
       "101496                             0                                     0   \n",
       "101497                             0                                     0   \n",
       "101498                             0                                     0   \n",
       "101499                             2                                     0   \n",
       "101500                             0                                     0   \n",
       "101501                             2                                     1   \n",
       "101502                             0                                     0   \n",
       "\n",
       "        NumberOfDependents  \n",
       "101493                 2.0  \n",
       "101494                 0.0  \n",
       "101495                 2.0  \n",
       "101496                 0.0  \n",
       "101497                 0.0  \n",
       "101498                 0.0  \n",
       "101499                 4.0  \n",
       "101500                 NaN  \n",
       "101501                 3.0  \n",
       "101502                 0.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute to All variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data train impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Max Singular Value of X_init = 5498797.118077\n",
      "[SoftImpute] Iter 1: observed MAE=22.144330 rank=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dian\\Anaconda3\\lib\\site-packages\\fancyimpute-0.3.1-py3.6.egg\\fancyimpute\\soft_impute.py:100: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed MAE=22.144389 rank=2\n",
      "[SoftImpute] Iter 3: observed MAE=22.144398 rank=2\n",
      "[SoftImpute] Iter 4: observed MAE=22.144402 rank=2\n",
      "[SoftImpute] Iter 5: observed MAE=22.144406 rank=2\n",
      "[SoftImpute] Iter 6: observed MAE=22.144409 rank=2\n",
      "[SoftImpute] Iter 7: observed MAE=22.144413 rank=2\n",
      "[SoftImpute] Iter 8: observed MAE=22.144416 rank=2\n",
      "[SoftImpute] Iter 9: observed MAE=22.144420 rank=2\n",
      "[SoftImpute] Iter 10: observed MAE=22.144423 rank=2\n",
      "[SoftImpute] Iter 11: observed MAE=22.144426 rank=2\n",
      "[SoftImpute] Iter 12: observed MAE=22.144429 rank=2\n",
      "[SoftImpute] Iter 13: observed MAE=22.144432 rank=2\n",
      "[SoftImpute] Iter 14: observed MAE=22.144435 rank=2\n",
      "[SoftImpute] Iter 15: observed MAE=22.144438 rank=2\n",
      "[SoftImpute] Iter 16: observed MAE=22.144441 rank=2\n",
      "[SoftImpute] Iter 17: observed MAE=22.144444 rank=2\n",
      "[SoftImpute] Iter 18: observed MAE=22.144447 rank=2\n",
      "[SoftImpute] Iter 19: observed MAE=22.144450 rank=2\n",
      "[SoftImpute] Iter 20: observed MAE=22.144452 rank=2\n",
      "[SoftImpute] Iter 21: observed MAE=22.144455 rank=2\n",
      "[SoftImpute] Iter 22: observed MAE=22.144457 rank=2\n",
      "[SoftImpute] Iter 23: observed MAE=22.144460 rank=2\n",
      "[SoftImpute] Iter 24: observed MAE=22.144463 rank=2\n",
      "[SoftImpute] Iter 25: observed MAE=22.144465 rank=2\n",
      "[SoftImpute] Iter 26: observed MAE=22.144467 rank=2\n",
      "[SoftImpute] Iter 27: observed MAE=22.144470 rank=2\n",
      "[SoftImpute] Iter 28: observed MAE=22.144472 rank=2\n",
      "[SoftImpute] Iter 29: observed MAE=22.144474 rank=2\n",
      "[SoftImpute] Iter 30: observed MAE=22.144477 rank=2\n",
      "[SoftImpute] Iter 31: observed MAE=22.144479 rank=2\n",
      "[SoftImpute] Iter 32: observed MAE=22.144481 rank=2\n",
      "[SoftImpute] Iter 33: observed MAE=22.144483 rank=2\n",
      "[SoftImpute] Iter 34: observed MAE=22.144485 rank=2\n",
      "[SoftImpute] Iter 35: observed MAE=22.144487 rank=2\n",
      "[SoftImpute] Iter 36: observed MAE=22.144489 rank=2\n",
      "[SoftImpute] Iter 37: observed MAE=22.144491 rank=2\n",
      "[SoftImpute] Iter 38: observed MAE=22.144493 rank=2\n",
      "[SoftImpute] Iter 39: observed MAE=22.144495 rank=2\n",
      "[SoftImpute] Iter 40: observed MAE=22.144497 rank=2\n",
      "[SoftImpute] Iter 41: observed MAE=22.144499 rank=2\n",
      "[SoftImpute] Iter 42: observed MAE=22.144500 rank=2\n",
      "[SoftImpute] Iter 43: observed MAE=22.144502 rank=2\n",
      "[SoftImpute] Iter 44: observed MAE=22.144504 rank=2\n",
      "[SoftImpute] Iter 45: observed MAE=22.144506 rank=2\n",
      "[SoftImpute] Iter 46: observed MAE=22.144507 rank=2\n",
      "[SoftImpute] Iter 47: observed MAE=22.144509 rank=2\n",
      "[SoftImpute] Iter 48: observed MAE=22.144511 rank=2\n",
      "[SoftImpute] Iter 49: observed MAE=22.144512 rank=2\n",
      "[SoftImpute] Iter 50: observed MAE=22.144514 rank=2\n",
      "[SoftImpute] Iter 51: observed MAE=22.144515 rank=2\n",
      "[SoftImpute] Iter 52: observed MAE=22.144517 rank=2\n",
      "[SoftImpute] Iter 53: observed MAE=22.144518 rank=2\n",
      "[SoftImpute] Iter 54: observed MAE=22.144519 rank=2\n",
      "[SoftImpute] Iter 55: observed MAE=22.144521 rank=2\n",
      "[SoftImpute] Iter 56: observed MAE=22.144522 rank=2\n",
      "[SoftImpute] Iter 57: observed MAE=22.144524 rank=2\n",
      "[SoftImpute] Iter 58: observed MAE=22.144525 rank=2\n",
      "[SoftImpute] Iter 59: observed MAE=22.144526 rank=2\n",
      "[SoftImpute] Iter 60: observed MAE=22.144528 rank=2\n",
      "[SoftImpute] Iter 61: observed MAE=22.144529 rank=2\n",
      "[SoftImpute] Iter 62: observed MAE=22.144530 rank=2\n",
      "[SoftImpute] Iter 63: observed MAE=22.144531 rank=2\n",
      "[SoftImpute] Iter 64: observed MAE=22.144532 rank=2\n",
      "[SoftImpute] Iter 65: observed MAE=22.144534 rank=2\n",
      "[SoftImpute] Iter 66: observed MAE=22.144535 rank=2\n",
      "[SoftImpute] Iter 67: observed MAE=22.144536 rank=2\n",
      "[SoftImpute] Iter 68: observed MAE=22.144537 rank=2\n",
      "[SoftImpute] Iter 69: observed MAE=22.144538 rank=2\n",
      "[SoftImpute] Iter 70: observed MAE=22.144539 rank=2\n",
      "[SoftImpute] Iter 71: observed MAE=22.144540 rank=2\n",
      "[SoftImpute] Iter 72: observed MAE=22.144541 rank=2\n",
      "[SoftImpute] Iter 73: observed MAE=22.144542 rank=2\n",
      "[SoftImpute] Iter 74: observed MAE=22.144543 rank=2\n",
      "[SoftImpute] Iter 75: observed MAE=22.144544 rank=2\n",
      "[SoftImpute] Iter 76: observed MAE=22.144545 rank=2\n",
      "[SoftImpute] Iter 77: observed MAE=22.144546 rank=2\n",
      "[SoftImpute] Iter 78: observed MAE=22.144547 rank=2\n",
      "[SoftImpute] Iter 79: observed MAE=22.144548 rank=2\n",
      "[SoftImpute] Iter 80: observed MAE=22.144549 rank=2\n",
      "[SoftImpute] Iter 81: observed MAE=22.144549 rank=2\n",
      "[SoftImpute] Iter 82: observed MAE=22.144550 rank=2\n",
      "[SoftImpute] Iter 83: observed MAE=22.144551 rank=2\n",
      "[SoftImpute] Iter 84: observed MAE=22.144552 rank=2\n",
      "[SoftImpute] Iter 85: observed MAE=22.144553 rank=2\n",
      "[SoftImpute] Iter 86: observed MAE=22.144553 rank=2\n",
      "[SoftImpute] Iter 87: observed MAE=22.144554 rank=2\n",
      "[SoftImpute] Iter 88: observed MAE=22.144555 rank=2\n",
      "[SoftImpute] Iter 89: observed MAE=22.144556 rank=2\n",
      "[SoftImpute] Iter 90: observed MAE=22.144556 rank=2\n",
      "[SoftImpute] Iter 91: observed MAE=22.144557 rank=2\n",
      "[SoftImpute] Iter 92: observed MAE=22.144558 rank=2\n",
      "[SoftImpute] Iter 93: observed MAE=22.144559 rank=2\n",
      "[SoftImpute] Iter 94: observed MAE=22.144559 rank=2\n",
      "[SoftImpute] Iter 95: observed MAE=22.144560 rank=2\n",
      "[SoftImpute] Iter 96: observed MAE=22.144561 rank=2\n",
      "[SoftImpute] Iter 97: observed MAE=22.144561 rank=2\n",
      "[SoftImpute] Iter 98: observed MAE=22.144562 rank=2\n",
      "[SoftImpute] Iter 99: observed MAE=22.144562 rank=2\n",
      "[SoftImpute] Iter 100: observed MAE=22.144563 rank=2\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=109975.942362\n"
     ]
    }
   ],
   "source": [
    "training_dataset.iloc[:,1:] = SoftImpute().complete(training_dataset.iloc[:,1:])\n",
    "training_dataset = training_dataset.round({'age': 0, 'NumberOfTime30-59DaysPastDueNotWorse': 0, 'NumberOfOpenCreditLinesAndLoans':0,\n",
    "                            'NumberOfTimes90DaysLate':0, 'NumberRealEstateLoansOrLines':0, 'NumberOfTime60-89DaysPastDueNotWorse':0,\n",
    "                            'NumberOfDependents':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_dataset = training_dataset[training_dataset.age != 0]\n",
    "training_dataset = training_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213179</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375607</td>\n",
       "      <td>3500.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.305682</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5710.000000</td>\n",
       "      <td>2.323121</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.754464</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209940</td>\n",
       "      <td>3500.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116951</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.611424</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189169</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>23684.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0      0                 1                              0.766127  45.0   \n",
       "1      1                 0                              0.957151  40.0   \n",
       "2      2                 0                              0.658180  38.0   \n",
       "3      3                 0                              0.233810  30.0   \n",
       "4      4                 0                              0.907239  49.0   \n",
       "5      5                 0                              0.213179  74.0   \n",
       "6      6                 0                              0.305682  57.0   \n",
       "7      7                 0                              0.754464  39.0   \n",
       "8      8                 0                              0.116951  27.0   \n",
       "9      9                 0                              0.189169  57.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                   2.0     0.802982    9120.000000   \n",
       "1                                   0.0     0.121876    2600.000000   \n",
       "2                                   1.0     0.085113    3042.000000   \n",
       "3                                   0.0     0.036050    3300.000000   \n",
       "4                                   1.0     0.024926   63588.000000   \n",
       "5                                   0.0     0.375607    3500.000000   \n",
       "6                                   0.0  5710.000000       2.323121   \n",
       "7                                   0.0     0.209940    3500.000000   \n",
       "8                                   0.0    46.000000       1.611424   \n",
       "9                                   0.0     0.606291   23684.000000   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                             13.0                      0.0   \n",
       "1                              4.0                      0.0   \n",
       "2                              2.0                      1.0   \n",
       "3                              5.0                      0.0   \n",
       "4                              7.0                      0.0   \n",
       "5                              3.0                      0.0   \n",
       "6                              8.0                      0.0   \n",
       "7                              8.0                      0.0   \n",
       "8                              2.0                      0.0   \n",
       "9                              9.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           6.0                                   0.0   \n",
       "1                           0.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           0.0                                   0.0   \n",
       "4                           1.0                                   0.0   \n",
       "5                           1.0                                   0.0   \n",
       "6                           3.0                                   0.0   \n",
       "7                           0.0                                   0.0   \n",
       "8                           0.0                                   0.0   \n",
       "9                           4.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "5                 1.0  \n",
       "6                 0.0  \n",
       "7                 0.0  \n",
       "8                 0.0  \n",
       "9                 2.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Test Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Max Singular Value of X_init = 10598115.191638\n",
      "[SoftImpute] Iter 1: observed MAE=32.035881 rank=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dian\\Anaconda3\\lib\\site-packages\\fancyimpute-0.3.1-py3.6.egg\\fancyimpute\\soft_impute.py:100: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SoftImpute] Iter 2: observed MAE=32.035890 rank=2\n",
      "[SoftImpute] Iter 3: observed MAE=32.035891 rank=2\n",
      "[SoftImpute] Iter 4: observed MAE=32.035891 rank=2\n",
      "[SoftImpute] Iter 5: observed MAE=32.035891 rank=2\n",
      "[SoftImpute] Iter 6: observed MAE=32.035891 rank=2\n",
      "[SoftImpute] Iter 7: observed MAE=32.035892 rank=2\n",
      "[SoftImpute] Iter 8: observed MAE=32.035892 rank=2\n",
      "[SoftImpute] Iter 9: observed MAE=32.035892 rank=2\n",
      "[SoftImpute] Iter 10: observed MAE=32.035893 rank=2\n",
      "[SoftImpute] Iter 11: observed MAE=32.035893 rank=2\n",
      "[SoftImpute] Iter 12: observed MAE=32.035893 rank=2\n",
      "[SoftImpute] Iter 13: observed MAE=32.035893 rank=2\n",
      "[SoftImpute] Iter 14: observed MAE=32.035894 rank=2\n",
      "[SoftImpute] Iter 15: observed MAE=32.035894 rank=2\n",
      "[SoftImpute] Iter 16: observed MAE=32.035894 rank=2\n",
      "[SoftImpute] Iter 17: observed MAE=32.035894 rank=2\n",
      "[SoftImpute] Iter 18: observed MAE=32.035894 rank=2\n",
      "[SoftImpute] Iter 19: observed MAE=32.035895 rank=2\n",
      "[SoftImpute] Iter 20: observed MAE=32.035895 rank=2\n",
      "[SoftImpute] Iter 21: observed MAE=32.035895 rank=2\n",
      "[SoftImpute] Iter 22: observed MAE=32.035895 rank=2\n",
      "[SoftImpute] Iter 23: observed MAE=32.035896 rank=2\n",
      "[SoftImpute] Iter 24: observed MAE=32.035896 rank=2\n",
      "[SoftImpute] Iter 25: observed MAE=32.035896 rank=2\n",
      "[SoftImpute] Iter 26: observed MAE=32.035896 rank=2\n",
      "[SoftImpute] Iter 27: observed MAE=32.035896 rank=2\n",
      "[SoftImpute] Iter 28: observed MAE=32.035897 rank=2\n",
      "[SoftImpute] Iter 29: observed MAE=32.035897 rank=2\n",
      "[SoftImpute] Iter 30: observed MAE=32.035897 rank=2\n",
      "[SoftImpute] Iter 31: observed MAE=32.035897 rank=2\n",
      "[SoftImpute] Iter 32: observed MAE=32.035897 rank=2\n",
      "[SoftImpute] Iter 33: observed MAE=32.035897 rank=2\n",
      "[SoftImpute] Iter 34: observed MAE=32.035898 rank=2\n",
      "[SoftImpute] Iter 35: observed MAE=32.035898 rank=2\n",
      "[SoftImpute] Iter 36: observed MAE=32.035898 rank=2\n",
      "[SoftImpute] Iter 37: observed MAE=32.035898 rank=2\n",
      "[SoftImpute] Iter 38: observed MAE=32.035898 rank=2\n",
      "[SoftImpute] Iter 39: observed MAE=32.035898 rank=2\n",
      "[SoftImpute] Iter 40: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 41: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 42: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 43: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 44: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 45: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 46: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 47: observed MAE=32.035899 rank=2\n",
      "[SoftImpute] Iter 48: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 49: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 50: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 51: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 52: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 53: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 54: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 55: observed MAE=32.035900 rank=2\n",
      "[SoftImpute] Iter 56: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 57: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 58: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 59: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 60: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 61: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 62: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 63: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 64: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 65: observed MAE=32.035901 rank=2\n",
      "[SoftImpute] Iter 66: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 67: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 68: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 69: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 70: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 71: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 72: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 73: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 74: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 75: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 76: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 77: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 78: observed MAE=32.035902 rank=2\n",
      "[SoftImpute] Iter 79: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 80: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 81: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 82: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 83: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 84: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 85: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 86: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 87: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 88: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 89: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 90: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 91: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 92: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 93: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 94: observed MAE=32.035903 rank=2\n",
      "[SoftImpute] Iter 95: observed MAE=32.035904 rank=2\n",
      "[SoftImpute] Iter 96: observed MAE=32.035904 rank=2\n",
      "[SoftImpute] Iter 97: observed MAE=32.035904 rank=2\n",
      "[SoftImpute] Iter 98: observed MAE=32.035904 rank=2\n",
      "[SoftImpute] Iter 99: observed MAE=32.035904 rank=2\n",
      "[SoftImpute] Iter 100: observed MAE=32.035904 rank=2\n",
      "[SoftImpute] Stopped after iteration 100 for lambda=211962.303833\n"
     ]
    }
   ],
   "source": [
    "test_dataset.iloc[:,1:] = SoftImpute().complete(test_dataset.iloc[:,1:])\n",
    "test_dataset = test_dataset.round({'age': 0, 'NumberOfTime30-59DaysPastDueNotWorse': 0, 'NumberOfOpenCreditLinesAndLoans':0,\n",
    "                            'NumberOfTimes90DaysLate':0, 'NumberRealEstateLoansOrLines':0, 'NumberOfTime60-89DaysPastDueNotWorse':0,\n",
    "                            'NumberOfDependents':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.885519</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177513</td>\n",
       "      <td>5700.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.463295</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527237</td>\n",
       "      <td>9141.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043275</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.687648</td>\n",
       "      <td>5083.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.280308</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925961</td>\n",
       "      <td>3200.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>3865.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509791</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342429</td>\n",
       "      <td>4140.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.587778</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1048.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046149</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.369170</td>\n",
       "      <td>3301.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>0.725155</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0               NaN                              0.885519  43.0   \n",
       "1               NaN                              0.463295  57.0   \n",
       "2               NaN                              0.043275  59.0   \n",
       "3               NaN                              0.280308  38.0   \n",
       "4               NaN                              1.000000  27.0   \n",
       "5               NaN                              0.509791  63.0   \n",
       "6               NaN                              0.587778  50.0   \n",
       "7               NaN                              0.046149  79.0   \n",
       "8               NaN                              0.013527  68.0   \n",
       "9               NaN                              1.000000  23.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
       "0                                   0.0     0.177513    5700.000000   \n",
       "1                                   0.0     0.527237    9141.000000   \n",
       "2                                   0.0     0.687648    5083.000000   \n",
       "3                                   1.0     0.925961    3200.000000   \n",
       "4                                   0.0     0.019917    3865.000000   \n",
       "5                                   0.0     0.342429    4140.000000   \n",
       "6                                   0.0  1048.000000       0.000000   \n",
       "7                                   1.0     0.369170    3301.000000   \n",
       "8                                   0.0  2024.000000       0.725155   \n",
       "9                                  98.0     0.000000       0.000000   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                              4.0                      0.0   \n",
       "1                             15.0                      0.0   \n",
       "2                             12.0                      0.0   \n",
       "3                              7.0                      0.0   \n",
       "4                              4.0                      0.0   \n",
       "5                              4.0                      0.0   \n",
       "6                              5.0                      0.0   \n",
       "7                              8.0                      0.0   \n",
       "8                              4.0                      0.0   \n",
       "9                              0.0                     98.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           0.0                                   0.0   \n",
       "1                           4.0                                   0.0   \n",
       "2                           1.0                                   0.0   \n",
       "3                           2.0                                   0.0   \n",
       "4                           0.0                                   0.0   \n",
       "5                           0.0                                   0.0   \n",
       "6                           0.0                                   0.0   \n",
       "7                           1.0                                   0.0   \n",
       "8                           1.0                                   0.0   \n",
       "9                           0.0                                  98.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 0.0  \n",
       "1                 2.0  \n",
       "2                 2.0  \n",
       "3                 0.0  \n",
       "4                 1.0  \n",
       "5                 1.0  \n",
       "6                 3.0  \n",
       "7                 1.0  \n",
       "8                 0.0  \n",
       "9                 0.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalitation Variable age and MonthlyIncome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalling variable age and MonthlyIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_size = training_dataset.values.shape[0]\n",
    "test_size = test_dataset.values.shape[0]\n",
    "training_dataset['age'] = scaler.fit_transform(training_dataset['age'].values.reshape([train_size,-1]))\n",
    "test_dataset['age'] = scaler.transform(test_dataset['age'].values.reshape([test_size,-1]))\n",
    "scaler = StandardScaler()\n",
    "training_dataset['MonthlyIncome'] = scaler.fit_transform(training_dataset['MonthlyIncome'].values.reshape([train_size,-1]))\n",
    "test_dataset['MonthlyIncome'] = scaler.transform(test_dataset['MonthlyIncome'].values.reshape([test_size,-1]))\n",
    "scaler = StandardScaler()\n",
    "training_dataset['DebtRatio'] = scaler.fit_transform(training_dataset['DebtRatio'].values.reshape([train_size,-1]))\n",
    "test_dataset['DebtRatio'] = scaler.transform(test_dataset['DebtRatio'].values.reshape([test_size,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>-0.493902</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.172834</td>\n",
       "      <td>0.286747</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>-0.832398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.173168</td>\n",
       "      <td>-0.209003</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>-0.967796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.173186</td>\n",
       "      <td>-0.175395</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>-1.509389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.173210</td>\n",
       "      <td>-0.155778</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>-0.223106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.173216</td>\n",
       "      <td>4.428232</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213179</td>\n",
       "      <td>1.469371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.173044</td>\n",
       "      <td>-0.140571</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.305682</td>\n",
       "      <td>0.318487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.628788</td>\n",
       "      <td>-0.406517</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.754464</td>\n",
       "      <td>-0.900097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.173125</td>\n",
       "      <td>-0.140571</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116951</td>\n",
       "      <td>-1.712486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150655</td>\n",
       "      <td>-0.406572</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189169</td>\n",
       "      <td>0.318487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.172931</td>\n",
       "      <td>1.394124</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines       age  \\\n",
       "0      0                 1                              0.766127 -0.493902   \n",
       "1      1                 0                              0.957151 -0.832398   \n",
       "2      2                 0                              0.658180 -0.967796   \n",
       "3      3                 0                              0.233810 -1.509389   \n",
       "4      4                 0                              0.907239 -0.223106   \n",
       "5      5                 0                              0.213179  1.469371   \n",
       "6      6                 0                              0.305682  0.318487   \n",
       "7      7                 0                              0.754464 -0.900097   \n",
       "8      8                 0                              0.116951 -1.712486   \n",
       "9      9                 0                              0.189169  0.318487   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                   2.0  -0.172834       0.286747   \n",
       "1                                   0.0  -0.173168      -0.209003   \n",
       "2                                   1.0  -0.173186      -0.175395   \n",
       "3                                   0.0  -0.173210      -0.155778   \n",
       "4                                   1.0  -0.173216       4.428232   \n",
       "5                                   0.0  -0.173044      -0.140571   \n",
       "6                                   0.0   2.628788      -0.406517   \n",
       "7                                   0.0  -0.173125      -0.140571   \n",
       "8                                   0.0  -0.150655      -0.406572   \n",
       "9                                   0.0  -0.172931       1.394124   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                             13.0                      0.0   \n",
       "1                              4.0                      0.0   \n",
       "2                              2.0                      1.0   \n",
       "3                              5.0                      0.0   \n",
       "4                              7.0                      0.0   \n",
       "5                              3.0                      0.0   \n",
       "6                              8.0                      0.0   \n",
       "7                              8.0                      0.0   \n",
       "8                              2.0                      0.0   \n",
       "9                              9.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           6.0                                   0.0   \n",
       "1                           0.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           0.0                                   0.0   \n",
       "4                           1.0                                   0.0   \n",
       "5                           1.0                                   0.0   \n",
       "6                           3.0                                   0.0   \n",
       "7                           0.0                                   0.0   \n",
       "8                           0.0                                   0.0   \n",
       "9                           4.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "5                 1.0  \n",
       "6                 0.0  \n",
       "7                 0.0  \n",
       "8                 0.0  \n",
       "9                 2.0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ['RevolvingUtilizationOfUnsecuredLines', 'age','NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome','NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents']\n",
    "X_train = training_dataset[X].values\n",
    "Y_train = training_dataset[['SeriousDlqin2yrs']].values\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train,Y_train, test_size = 0.2, random_state = 0)\n",
    "X_test = test_dataset[X].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran X_train :  (119999, 10)\n",
      "Ukuran Y_train :  (119999, 1)\n",
      "Ukuran X_test  :  (101503, 10)\n",
      "Ukuran X_val   :  (30000, 10)\n",
      "ukuran Y_val   :  (30000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Ukuran X_train : \",X_train.shape)\n",
    "print(\"Ukuran Y_train : \",Y_train.shape)\n",
    "print(\"Ukuran X_test  : \",X_test.shape)\n",
    "print(\"Ukuran X_val   : \",X_validation.shape)\n",
    "print(\"ukuran Y_val   : \",Y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Dataset linier or nonlinier case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96     27969\n",
      "          1       0.39      0.02      0.03      2031\n",
      "\n",
      "avg / total       0.90      0.93      0.90     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "Y_train = Y_train.reshape(-1)\n",
    "svc_linier = SVC(kernel='linear')\n",
    "svc_linier.fit(X_train[:5000], Y_train[:5000])\n",
    "SVC_linier_predict = svc_linier.predict(X_validation)\n",
    "print(classification_report(Y_validation, SVC_linier_predict, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97     27969\n",
      "          1       0.65      0.02      0.04      2031\n",
      "\n",
      "avg / total       0.91      0.93      0.90     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "Y_train = Y_train.reshape(-1)\n",
    "svc_linier = SVC(kernel='rbf')\n",
    "svc_linier.fit(X_train[:5000], Y_train[:5000])\n",
    "SVC_linier_predict = svc_linier.predict(X_validation)\n",
    "print(classification_report(Y_validation, SVC_linier_predict, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Linier or Non Linier*\n",
    "This case is non linier clasification, because prediction using SVM (kernel='rbf') better than SVM (kernel='linier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 600, input_dim = 10, activation = 'sigmoid'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999\n",
      "10\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/20\n",
      "119999/119999 [==============================] - 8s 65us/step - loss: 0.1893 - acc: 0.9371 - val_loss: 0.2022 - val_acc: 0.9353\n",
      "Epoch 2/20\n",
      "119999/119999 [==============================] - 5s 44us/step - loss: 0.1893 - acc: 0.9374 - val_loss: 0.2001 - val_acc: 0.9356\n",
      "Epoch 3/20\n",
      "119999/119999 [==============================] - 6s 48us/step - loss: 0.1882 - acc: 0.9376 - val_loss: 0.2017 - val_acc: 0.9350\n",
      "Epoch 4/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1883 - acc: 0.9377 - val_loss: 0.2032 - val_acc: 0.9354\n",
      "Epoch 5/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1877 - acc: 0.9378 - val_loss: 0.1999 - val_acc: 0.9353\n",
      "Epoch 6/20\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1875 - acc: 0.9378 - val_loss: 0.2009 - val_acc: 0.9351\n",
      "Epoch 7/20\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1873 - acc: 0.9378 - val_loss: 0.2023 - val_acc: 0.9353\n",
      "Epoch 8/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1875 - acc: 0.9380 - val_loss: 0.2021 - val_acc: 0.9349\n",
      "Epoch 9/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1870 - acc: 0.9380 - val_loss: 0.2010 - val_acc: 0.9351\n",
      "Epoch 10/20\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1868 - acc: 0.9380 - val_loss: 0.2014 - val_acc: 0.9352\n",
      "Epoch 11/20\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1875 - acc: 0.9379 - val_loss: 0.2022 - val_acc: 0.9347\n",
      "Epoch 12/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1868 - acc: 0.9384 - val_loss: 0.2039 - val_acc: 0.9351\n",
      "Epoch 13/20\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1869 - acc: 0.9383 - val_loss: 0.2020 - val_acc: 0.9350\n",
      "Epoch 14/20\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1867 - acc: 0.9384 - val_loss: 0.2027 - val_acc: 0.9350\n",
      "Epoch 15/20\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1864 - acc: 0.9384 - val_loss: 0.2032 - val_acc: 0.9349\n",
      "Epoch 16/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1866 - acc: 0.9383 - val_loss: 0.2068 - val_acc: 0.9348\n",
      "Epoch 17/20\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1863 - acc: 0.9382 - val_loss: 0.2043 - val_acc: 0.9349\n",
      "Epoch 18/20\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1866 - acc: 0.9386 - val_loss: 0.2042 - val_acc: 0.9349\n",
      "Epoch 19/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1863 - acc: 0.9388 - val_loss: 0.2035 - val_acc: 0.9344\n",
      "Epoch 20/20\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1880 - acc: 0.9382 - val_loss: 0.2033 - val_acc: 0.9350\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_train.shape[1])\n",
    "log = model.fit(X_train, Y_train, batch_size= 5000, epochs=20, verbose=1,validation_data=(X_validation, Y_validation))  # starts training\n",
    "#model = model.fit(X_train, Y_train, epochs=100, batch_size=X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 5s 170us/step\n",
      "\n",
      "acc: 93.50%\n",
      "\n",
      "loss: 20.33%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_validation, Y_validation)\n",
    "print('\\n%s: %.2f%%'%(model.metrics_names[1], scores[1]*100))\n",
    "print('\\n%s: %.2f%%'%(model.metrics_names[0], scores[0]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_neural_network_predict = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27752   192     3     1    21     0]\n",
      " [ 1699   297     0     0    32     3]\n",
      " [    0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_validation, [np.round(pred) for pred in shallow_neural_network_predict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_against_epoch(model):\n",
    "    plt.plot(model.history['acc'])\n",
    "    plt.plot(model.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "def plot_loss_against_epoch(model):\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqdjFkmzMvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2d1409438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk2xMQ+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f2d14094e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy_against_epoch(log)\n",
    "plot_loss_against_epoch(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "119999/119999 [==============================] - 12s 103us/step - loss: 0.3195 - acc: 0.8923 - val_loss: 0.2602 - val_acc: 0.9327\n",
      "Epoch 2/100\n",
      "119999/119999 [==============================] - 6s 48us/step - loss: 0.2254 - acc: 0.9350 - val_loss: 0.2078 - val_acc: 0.9341\n",
      "Epoch 3/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1968 - acc: 0.9361 - val_loss: 0.1919 - val_acc: 0.9347\n",
      "Epoch 4/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1934 - acc: 0.9360 - val_loss: 0.1925 - val_acc: 0.9347\n",
      "Epoch 5/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1870 - acc: 0.9363 - val_loss: 0.1851 - val_acc: 0.9357\n",
      "Epoch 6/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1823 - acc: 0.9365 - val_loss: 0.1829 - val_acc: 0.9356\n",
      "Epoch 7/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1811 - acc: 0.9369 - val_loss: 0.1822 - val_acc: 0.9358\n",
      "Epoch 8/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1817 - acc: 0.9371 - val_loss: 0.1825 - val_acc: 0.9352\n",
      "Epoch 9/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1797 - acc: 0.9375 - val_loss: 0.1810 - val_acc: 0.9357\n",
      "Epoch 10/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1797 - acc: 0.9373 - val_loss: 0.1809 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1793 - acc: 0.9377 - val_loss: 0.1825 - val_acc: 0.9357\n",
      "Epoch 12/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1791 - acc: 0.9378 - val_loss: 0.1806 - val_acc: 0.9368\n",
      "Epoch 13/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1784 - acc: 0.9379 - val_loss: 0.1811 - val_acc: 0.9367\n",
      "Epoch 14/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1799 - acc: 0.9376 - val_loss: 0.1863 - val_acc: 0.9345\n",
      "Epoch 15/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1811 - acc: 0.9369 - val_loss: 0.1832 - val_acc: 0.9353\n",
      "Epoch 16/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1796 - acc: 0.9375 - val_loss: 0.1826 - val_acc: 0.9356\n",
      "Epoch 17/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1788 - acc: 0.9379 - val_loss: 0.1806 - val_acc: 0.9363\n",
      "Epoch 18/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1783 - acc: 0.9378 - val_loss: 0.1814 - val_acc: 0.9363\n",
      "Epoch 19/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1786 - acc: 0.9380 - val_loss: 0.1836 - val_acc: 0.9356\n",
      "Epoch 20/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1794 - acc: 0.9378 - val_loss: 0.1821 - val_acc: 0.9362\n",
      "Epoch 21/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1786 - acc: 0.9377 - val_loss: 0.1807 - val_acc: 0.9366\n",
      "Epoch 22/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1774 - acc: 0.9378 - val_loss: 0.1808 - val_acc: 0.9363\n",
      "Epoch 23/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1775 - acc: 0.9380 - val_loss: 0.1818 - val_acc: 0.9355\n",
      "Epoch 24/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1778 - acc: 0.9380 - val_loss: 0.1810 - val_acc: 0.9354\n",
      "Epoch 25/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1770 - acc: 0.9379 - val_loss: 0.1808 - val_acc: 0.9357\n",
      "Epoch 26/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1770 - acc: 0.9382 - val_loss: 0.1808 - val_acc: 0.9361\n",
      "Epoch 27/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1775 - acc: 0.9381 - val_loss: 0.1836 - val_acc: 0.9354\n",
      "Epoch 28/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1777 - acc: 0.9381 - val_loss: 0.1825 - val_acc: 0.9359\n",
      "Epoch 29/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1771 - acc: 0.9382 - val_loss: 0.1814 - val_acc: 0.9363\n",
      "Epoch 30/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1769 - acc: 0.9381 - val_loss: 0.1818 - val_acc: 0.9354\n",
      "Epoch 31/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1773 - acc: 0.9381 - val_loss: 0.1815 - val_acc: 0.9366\n",
      "Epoch 32/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1763 - acc: 0.9381 - val_loss: 0.1810 - val_acc: 0.9361\n",
      "Epoch 33/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1763 - acc: 0.9382 - val_loss: 0.1832 - val_acc: 0.9350\n",
      "Epoch 34/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1772 - acc: 0.9380 - val_loss: 0.1854 - val_acc: 0.9352\n",
      "Epoch 35/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9383 - val_loss: 0.1809 - val_acc: 0.9366\n",
      "Epoch 36/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1761 - acc: 0.9382 - val_loss: 0.1808 - val_acc: 0.9361\n",
      "Epoch 37/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1761 - acc: 0.9380 - val_loss: 0.1814 - val_acc: 0.9353\n",
      "Epoch 38/100\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1762 - acc: 0.9381 - val_loss: 0.1815 - val_acc: 0.9358\n",
      "Epoch 39/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1787 - acc: 0.9381 - val_loss: 0.1834 - val_acc: 0.9358\n",
      "Epoch 40/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1795 - acc: 0.9378 - val_loss: 0.1831 - val_acc: 0.9364\n",
      "Epoch 41/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1802 - acc: 0.9379 - val_loss: 0.1839 - val_acc: 0.9358\n",
      "Epoch 42/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1775 - acc: 0.9382 - val_loss: 0.1813 - val_acc: 0.9360\n",
      "Epoch 43/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1769 - acc: 0.9383 - val_loss: 0.1819 - val_acc: 0.9356\n",
      "Epoch 44/100\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1762 - acc: 0.9385 - val_loss: 0.1836 - val_acc: 0.9357\n",
      "Epoch 45/100\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1768 - acc: 0.9384 - val_loss: 0.1835 - val_acc: 0.9355\n",
      "Epoch 46/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1775 - acc: 0.9382 - val_loss: 0.1818 - val_acc: 0.9361\n",
      "Epoch 47/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1761 - acc: 0.9383 - val_loss: 0.1821 - val_acc: 0.9354\n",
      "Epoch 48/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1760 - acc: 0.9385 - val_loss: 0.1813 - val_acc: 0.9359\n",
      "Epoch 49/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1757 - acc: 0.9384 - val_loss: 0.1826 - val_acc: 0.9364\n",
      "Epoch 50/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1763 - acc: 0.9383 - val_loss: 0.1825 - val_acc: 0.9356\n",
      "Epoch 51/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1767 - acc: 0.9386 - val_loss: 0.1830 - val_acc: 0.9355\n",
      "Epoch 52/100\n",
      "119999/119999 [==============================] - 8s 69us/step - loss: 0.1768 - acc: 0.9381 - val_loss: 0.1814 - val_acc: 0.9351\n",
      "Epoch 53/100\n",
      "119999/119999 [==============================] - 8s 65us/step - loss: 0.1762 - acc: 0.9383 - val_loss: 0.1844 - val_acc: 0.9355\n",
      "Epoch 54/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1756 - acc: 0.9388 - val_loss: 0.1813 - val_acc: 0.9355\n",
      "Epoch 55/100\n",
      "119999/119999 [==============================] - 8s 68us/step - loss: 0.1752 - acc: 0.9389 - val_loss: 0.1819 - val_acc: 0.9353\n",
      "Epoch 56/100\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1750 - acc: 0.9387 - val_loss: 0.1813 - val_acc: 0.9362\n",
      "Epoch 57/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1747 - acc: 0.9387 - val_loss: 0.1827 - val_acc: 0.9356\n",
      "Epoch 58/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1748 - acc: 0.9388 - val_loss: 0.1826 - val_acc: 0.9357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1749 - acc: 0.9386 - val_loss: 0.1817 - val_acc: 0.9361\n",
      "Epoch 60/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1756 - acc: 0.9387 - val_loss: 0.1831 - val_acc: 0.9351\n",
      "Epoch 61/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1752 - acc: 0.9389 - val_loss: 0.1819 - val_acc: 0.9363\n",
      "Epoch 62/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1745 - acc: 0.9391 - val_loss: 0.1824 - val_acc: 0.9353\n",
      "Epoch 63/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1748 - acc: 0.9391 - val_loss: 0.1839 - val_acc: 0.9351\n",
      "Epoch 64/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1751 - acc: 0.9391 - val_loss: 0.1820 - val_acc: 0.9354\n",
      "Epoch 65/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1749 - acc: 0.9391 - val_loss: 0.1865 - val_acc: 0.9354\n",
      "Epoch 66/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1758 - acc: 0.9388 - val_loss: 0.1818 - val_acc: 0.9353\n",
      "Epoch 67/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1738 - acc: 0.9394 - val_loss: 0.1819 - val_acc: 0.9358\n",
      "Epoch 68/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1739 - acc: 0.9392 - val_loss: 0.1827 - val_acc: 0.9360\n",
      "Epoch 69/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1745 - acc: 0.9390 - val_loss: 0.1824 - val_acc: 0.9354\n",
      "Epoch 70/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1743 - acc: 0.9393 - val_loss: 0.1828 - val_acc: 0.9356\n",
      "Epoch 71/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1737 - acc: 0.9394 - val_loss: 0.1823 - val_acc: 0.9351\n",
      "Epoch 72/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1732 - acc: 0.9392 - val_loss: 0.1822 - val_acc: 0.9351\n",
      "Epoch 73/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1731 - acc: 0.9394 - val_loss: 0.1843 - val_acc: 0.9347\n",
      "Epoch 74/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1740 - acc: 0.9391 - val_loss: 0.1828 - val_acc: 0.9347\n",
      "Epoch 75/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1729 - acc: 0.9397 - val_loss: 0.1832 - val_acc: 0.9356\n",
      "Epoch 76/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1729 - acc: 0.9396 - val_loss: 0.1833 - val_acc: 0.9353\n",
      "Epoch 77/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1733 - acc: 0.9398 - val_loss: 0.1839 - val_acc: 0.9354\n",
      "Epoch 78/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1728 - acc: 0.9398 - val_loss: 0.1832 - val_acc: 0.9346\n",
      "Epoch 79/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1745 - acc: 0.9394 - val_loss: 0.1834 - val_acc: 0.9355\n",
      "Epoch 80/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1733 - acc: 0.9397 - val_loss: 0.1830 - val_acc: 0.9350\n",
      "Epoch 81/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1726 - acc: 0.9400 - val_loss: 0.1834 - val_acc: 0.9351\n",
      "Epoch 82/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1733 - acc: 0.9397 - val_loss: 0.1843 - val_acc: 0.9348\n",
      "Epoch 83/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1729 - acc: 0.9399 - val_loss: 0.1834 - val_acc: 0.9355\n",
      "Epoch 84/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1726 - acc: 0.9400 - val_loss: 0.1835 - val_acc: 0.9357\n",
      "Epoch 85/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1719 - acc: 0.9397 - val_loss: 0.1836 - val_acc: 0.9347\n",
      "Epoch 86/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1715 - acc: 0.9401 - val_loss: 0.1845 - val_acc: 0.9354\n",
      "Epoch 87/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1718 - acc: 0.9403 - val_loss: 0.1835 - val_acc: 0.9351\n",
      "Epoch 88/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1713 - acc: 0.9403 - val_loss: 0.1840 - val_acc: 0.9345\n",
      "Epoch 89/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1714 - acc: 0.9404 - val_loss: 0.1848 - val_acc: 0.9339\n",
      "Epoch 90/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1713 - acc: 0.9404 - val_loss: 0.1848 - val_acc: 0.9345\n",
      "Epoch 91/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1713 - acc: 0.9407 - val_loss: 0.1841 - val_acc: 0.9351\n",
      "Epoch 92/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1711 - acc: 0.9407 - val_loss: 0.1859 - val_acc: 0.9338\n",
      "Epoch 93/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1717 - acc: 0.9402 - val_loss: 0.1836 - val_acc: 0.9354\n",
      "Epoch 94/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1712 - acc: 0.9406 - val_loss: 0.1873 - val_acc: 0.9347\n",
      "Epoch 95/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1729 - acc: 0.9407 - val_loss: 0.1844 - val_acc: 0.9347\n",
      "Epoch 96/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1712 - acc: 0.9406 - val_loss: 0.1863 - val_acc: 0.9346\n",
      "Epoch 97/100\n",
      "119999/119999 [==============================] - 8s 67us/step - loss: 0.1717 - acc: 0.9409 - val_loss: 0.1856 - val_acc: 0.9352\n",
      "Epoch 98/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1703 - acc: 0.9412 - val_loss: 0.1859 - val_acc: 0.9343\n",
      "Epoch 99/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1702 - acc: 0.9412 - val_loss: 0.1851 - val_acc: 0.9348\n",
      "Epoch 100/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1699 - acc: 0.9418 - val_loss: 0.1858 - val_acc: 0.9349\n"
     ]
    }
   ],
   "source": [
    "dnn_model = Sequential()\n",
    "dnn_model.add(Dense(units = 600, input_dim = 10, activation = 'relu'))\n",
    "dnn_model.add(Dense(units = 300, activation = 'relu'))\n",
    "dnn_model.add(Dense(units = 100, activation = 'relu'))\n",
    "dnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "dnn_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "dnn_log = dnn_model.fit(X_train, Y_train, batch_size= 6000, epochs=100, verbose=1,\n",
    "                validation_data=(X_validation, Y_validation))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 4s 137us/step\n",
      "\n",
      "acc: 93.49%\n",
      "\n",
      "loss: 18.58%\n"
     ]
    }
   ],
   "source": [
    "scores = dnn_model.evaluate(X_validation, Y_validation)\n",
    "print('\\n%s: %.2f%%'%(model.metrics_names[1], scores[1]*100))\n",
    "print('\\n%s: %.2f%%'%(model.metrics_names[0], scores[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_neural_network_predict = dnn_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27687   282]\n",
      " [ 1672   359]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_validation, [np.round(pred) for pred in deep_neural_network_predict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAgZAhvAABChvAGACBk/j8C33BKjBL+uwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c41d7a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGwCAkCG8AQAIGcIbAICQIbwBAAiZ/x8dJHt0XQJz5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c41ecb668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy_against_epoch(log)\n",
    "plot_loss_against_epoch(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ['RevolvingUtilizationOfUnsecuredLines', 'age','NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome','NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse','NumberOfDependents']\n",
    "X_test = test_dataset[X].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_neural_network_predict = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.97     27957\n",
      "          1       0.53      0.19      0.28      2043\n",
      "\n",
      "avg / total       0.92      0.93      0.92     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_validation_raw, [np.round(pred) for pred in deep_neural_network_predict], labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_predict = pd.DataFrame(model.predict(X_test))\n",
    "Y_predict['Id'] = range(1, len(Y_predict) + 1)\n",
    "Y_predict['probability'] = Y_predict.iloc[:,[0]]\n",
    "Y_predict = Y_predict.iloc[:,1:]\n",
    "Y_predict.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.076624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.062004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.011756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.075529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.176861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.043615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.022698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.304449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.015003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.015390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.012093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.087274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.049667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.028926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.012459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.230821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.162658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.010895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.011840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.008981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.591227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.099659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.015463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.094141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.008787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.054327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.021290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101473</th>\n",
       "      <td>101474</td>\n",
       "      <td>0.052130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101474</th>\n",
       "      <td>101475</td>\n",
       "      <td>0.209093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101475</th>\n",
       "      <td>101476</td>\n",
       "      <td>0.150543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101476</th>\n",
       "      <td>101477</td>\n",
       "      <td>0.006410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101477</th>\n",
       "      <td>101478</td>\n",
       "      <td>0.005194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101478</th>\n",
       "      <td>101479</td>\n",
       "      <td>0.007759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101479</th>\n",
       "      <td>101480</td>\n",
       "      <td>0.158393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101480</th>\n",
       "      <td>101481</td>\n",
       "      <td>0.037225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101481</th>\n",
       "      <td>101482</td>\n",
       "      <td>0.014263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101482</th>\n",
       "      <td>101483</td>\n",
       "      <td>0.310386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101483</th>\n",
       "      <td>101484</td>\n",
       "      <td>0.031279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101484</th>\n",
       "      <td>101485</td>\n",
       "      <td>0.013059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101485</th>\n",
       "      <td>101486</td>\n",
       "      <td>0.008745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101486</th>\n",
       "      <td>101487</td>\n",
       "      <td>0.023579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101487</th>\n",
       "      <td>101488</td>\n",
       "      <td>0.004216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101488</th>\n",
       "      <td>101489</td>\n",
       "      <td>0.006085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101489</th>\n",
       "      <td>101490</td>\n",
       "      <td>0.162291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101490</th>\n",
       "      <td>101491</td>\n",
       "      <td>0.622601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101491</th>\n",
       "      <td>101492</td>\n",
       "      <td>0.058534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101492</th>\n",
       "      <td>101493</td>\n",
       "      <td>0.083081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101493</th>\n",
       "      <td>101494</td>\n",
       "      <td>0.004101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101494</th>\n",
       "      <td>101495</td>\n",
       "      <td>0.017856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101495</th>\n",
       "      <td>101496</td>\n",
       "      <td>0.368852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101496</th>\n",
       "      <td>101497</td>\n",
       "      <td>0.007013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101497</th>\n",
       "      <td>101498</td>\n",
       "      <td>0.003904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101498</th>\n",
       "      <td>101499</td>\n",
       "      <td>0.047096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101499</th>\n",
       "      <td>101500</td>\n",
       "      <td>0.402797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101500</th>\n",
       "      <td>101501</td>\n",
       "      <td>0.004718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101501</th>\n",
       "      <td>101502</td>\n",
       "      <td>0.043616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101502</th>\n",
       "      <td>101503</td>\n",
       "      <td>0.060175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101503 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  probability\n",
       "0            1     0.076624\n",
       "1            2     0.062004\n",
       "2            3     0.011756\n",
       "3            4     0.075529\n",
       "4            5     0.176861\n",
       "5            6     0.043615\n",
       "6            7     0.022698\n",
       "7            8     0.064500\n",
       "8            9     0.002832\n",
       "9           10     0.304449\n",
       "10          11     0.015003\n",
       "11          12     0.015390\n",
       "12          13     0.012093\n",
       "13          14     0.087274\n",
       "14          15     0.049667\n",
       "15          16     0.015575\n",
       "16          17     0.028926\n",
       "17          18     0.012459\n",
       "18          19     0.230821\n",
       "19          20     0.162658\n",
       "20          21     0.010895\n",
       "21          22     0.011840\n",
       "22          23     0.008981\n",
       "23          24     0.591227\n",
       "24          25     0.099659\n",
       "25          26     0.015463\n",
       "26          27     0.094141\n",
       "27          28     0.008787\n",
       "28          29     0.054327\n",
       "29          30     0.021290\n",
       "...        ...          ...\n",
       "101473  101474     0.052130\n",
       "101474  101475     0.209093\n",
       "101475  101476     0.150543\n",
       "101476  101477     0.006410\n",
       "101477  101478     0.005194\n",
       "101478  101479     0.007759\n",
       "101479  101480     0.158393\n",
       "101480  101481     0.037225\n",
       "101481  101482     0.014263\n",
       "101482  101483     0.310386\n",
       "101483  101484     0.031279\n",
       "101484  101485     0.013059\n",
       "101485  101486     0.008745\n",
       "101486  101487     0.023579\n",
       "101487  101488     0.004216\n",
       "101488  101489     0.006085\n",
       "101489  101490     0.162291\n",
       "101490  101491     0.622601\n",
       "101491  101492     0.058534\n",
       "101492  101493     0.083081\n",
       "101493  101494     0.004101\n",
       "101494  101495     0.017856\n",
       "101495  101496     0.368852\n",
       "101496  101497     0.007013\n",
       "101497  101498     0.003904\n",
       "101498  101499     0.047096\n",
       "101499  101500     0.402797\n",
       "101500  101501     0.004718\n",
       "101501  101502     0.043616\n",
       "101502  101503     0.060175\n",
       "\n",
       "[101503 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/10\n",
      "119999/119999 [==============================] - 12s 97us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 2/10\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 3/10\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 4/10\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 5/10\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 6/10\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 7/10\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 8/10\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 9/10\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 10/10\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/50\n",
      "119999/119999 [==============================] - 11s 94us/step - loss: 1.2365 - acc: 0.8840 - val_loss: 0.6242 - val_acc: 0.9160\n",
      "Epoch 2/50\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.4960 - acc: 0.9215 - val_loss: 0.4555 - val_acc: 0.9329\n",
      "Epoch 3/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.5631 - acc: 0.9277 - val_loss: 0.6215 - val_acc: 0.9326\n",
      "Epoch 4/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.3383 - acc: 0.9304 - val_loss: 0.2381 - val_acc: 0.9313\n",
      "Epoch 5/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2732 - acc: 0.9338 - val_loss: 0.2543 - val_acc: 0.9319\n",
      "Epoch 6/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0649 - acc: 0.8208 - val_loss: 0.9753 - val_acc: 0.9302\n",
      "Epoch 7/50\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.9060 - acc: 0.9301 - val_loss: 0.8341 - val_acc: 0.9175\n",
      "Epoch 8/50\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.6480 - acc: 0.9248 - val_loss: 0.3287 - val_acc: 0.9290\n",
      "Epoch 9/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.4344 - acc: 0.9237 - val_loss: 0.5582 - val_acc: 0.9310\n",
      "Epoch 10/50\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4393 - acc: 0.9319 - val_loss: 0.4574 - val_acc: 0.9309\n",
      "Epoch 11/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.3923 - acc: 0.9182 - val_loss: 0.3213 - val_acc: 0.9318\n",
      "Epoch 12/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2943 - acc: 0.9332 - val_loss: 0.2979 - val_acc: 0.9313\n",
      "Epoch 13/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.5501 - acc: 0.8885 - val_loss: 0.4543 - val_acc: 0.9313\n",
      "Epoch 14/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.3257 - acc: 0.9285 - val_loss: 0.2666 - val_acc: 0.9305\n",
      "Epoch 15/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2870 - acc: 0.9322 - val_loss: 0.3071 - val_acc: 0.9287\n",
      "Epoch 16/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.5259 - acc: 0.8340 - val_loss: 0.3577 - val_acc: 0.9036\n",
      "Epoch 17/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.3754 - acc: 0.9309 - val_loss: 0.2963 - val_acc: 0.9289\n",
      "Epoch 18/50\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3029 - acc: 0.9292 - val_loss: 0.3327 - val_acc: 0.9313\n",
      "Epoch 19/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.4826 - acc: 0.8943 - val_loss: 0.3555 - val_acc: 0.9333\n",
      "Epoch 20/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.3887 - acc: 0.8958 - val_loss: 0.3445 - val_acc: 0.9302\n",
      "Epoch 21/50\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.3420 - acc: 0.9327 - val_loss: 0.2512 - val_acc: 0.9333\n",
      "Epoch 22/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2387 - acc: 0.9340 - val_loss: 0.2309 - val_acc: 0.9315\n",
      "Epoch 23/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2504 - acc: 0.9337 - val_loss: 0.2728 - val_acc: 0.9314\n",
      "Epoch 24/50\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.2325 - acc: 0.9339 - val_loss: 0.2282 - val_acc: 0.9328\n",
      "Epoch 25/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2252 - acc: 0.9341 - val_loss: 0.2284 - val_acc: 0.9327\n",
      "Epoch 26/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2235 - acc: 0.9342 - val_loss: 0.2248 - val_acc: 0.9330\n",
      "Epoch 27/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2288 - acc: 0.9346 - val_loss: 0.2311 - val_acc: 0.9332\n",
      "Epoch 28/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2297 - acc: 0.9340 - val_loss: 0.2337 - val_acc: 0.9328\n",
      "Epoch 29/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2367 - acc: 0.9342 - val_loss: 0.2860 - val_acc: 0.9315\n",
      "Epoch 30/50\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2454 - acc: 0.9338 - val_loss: 0.2452 - val_acc: 0.9328\n",
      "Epoch 31/50\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2306 - acc: 0.9340 - val_loss: 0.2314 - val_acc: 0.9325\n",
      "Epoch 32/50\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2224 - acc: 0.9341 - val_loss: 0.2227 - val_acc: 0.9331\n",
      "Epoch 33/50\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2216 - acc: 0.9345 - val_loss: 0.2227 - val_acc: 0.9329\n",
      "Epoch 34/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2203 - acc: 0.9345 - val_loss: 0.2248 - val_acc: 0.9330\n",
      "Epoch 35/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2386 - acc: 0.9344 - val_loss: 0.2286 - val_acc: 0.9320\n",
      "Epoch 36/50\n",
      "119999/119999 [==============================] - 8s 71us/step - loss: 0.2215 - acc: 0.9345 - val_loss: 0.2240 - val_acc: 0.9330\n",
      "Epoch 37/50\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2202 - acc: 0.9344 - val_loss: 0.2286 - val_acc: 0.9333\n",
      "Epoch 38/50\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2195 - acc: 0.9345 - val_loss: 0.2222 - val_acc: 0.9331\n",
      "Epoch 39/50\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2239 - acc: 0.9351 - val_loss: 0.2983 - val_acc: 0.9315\n",
      "Epoch 40/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2924 - acc: 0.9240 - val_loss: 0.2228 - val_acc: 0.9324\n",
      "Epoch 41/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2396 - acc: 0.9343 - val_loss: 0.2334 - val_acc: 0.9333\n",
      "Epoch 42/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2275 - acc: 0.9348 - val_loss: 0.2068 - val_acc: 0.9330\n",
      "Epoch 43/50\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2101 - acc: 0.9358 - val_loss: 0.2111 - val_acc: 0.9326\n",
      "Epoch 44/50\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2144 - acc: 0.9356 - val_loss: 0.2436 - val_acc: 0.9339\n",
      "Epoch 45/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2381 - acc: 0.9311 - val_loss: 0.5014 - val_acc: 0.8416\n",
      "Epoch 46/50\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.3548 - acc: 0.9020 - val_loss: 0.2563 - val_acc: 0.9319\n",
      "Epoch 47/50\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2316 - acc: 0.9347 - val_loss: 0.2114 - val_acc: 0.9329\n",
      "Epoch 48/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2044 - acc: 0.9357 - val_loss: 0.2100 - val_acc: 0.9337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1993 - acc: 0.9358 - val_loss: 0.2022 - val_acc: 0.9345\n",
      "Epoch 50/50\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1971 - acc: 0.9359 - val_loss: 0.2005 - val_acc: 0.9343\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "119999/119999 [==============================] - 12s 98us/step - loss: 0.8173 - acc: 0.9191 - val_loss: 0.7324 - val_acc: 0.9247\n",
      "Epoch 2/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.5871 - acc: 0.9258 - val_loss: 0.4648 - val_acc: 0.9312\n",
      "Epoch 3/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.3484 - acc: 0.9326 - val_loss: 0.4020 - val_acc: 0.9313\n",
      "Epoch 4/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.8073 - acc: 0.8540 - val_loss: 0.3378 - val_acc: 0.9259\n",
      "Epoch 5/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.8499 - acc: 0.9316 - val_loss: 0.8275 - val_acc: 0.9283\n",
      "Epoch 6/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.5839 - acc: 0.9290 - val_loss: 0.2915 - val_acc: 0.9318\n",
      "Epoch 7/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.5250 - acc: 0.9215 - val_loss: 0.6227 - val_acc: 0.9315\n",
      "Epoch 8/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.3925 - acc: 0.9301 - val_loss: 0.2767 - val_acc: 0.9318\n",
      "Epoch 9/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.4929 - acc: 0.8990 - val_loss: 0.8840 - val_acc: 0.9301\n",
      "Epoch 10/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.7078 - acc: 0.8974 - val_loss: 1.0474 - val_acc: 0.6316\n",
      "Epoch 11/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.6530 - acc: 0.9062 - val_loss: 0.6950 - val_acc: 0.9307\n",
      "Epoch 12/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.5084 - acc: 0.9311 - val_loss: 0.3852 - val_acc: 0.9312\n",
      "Epoch 13/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3277 - acc: 0.9336 - val_loss: 0.2535 - val_acc: 0.9319\n",
      "Epoch 14/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2832 - acc: 0.9329 - val_loss: 0.3504 - val_acc: 0.9316\n",
      "Epoch 15/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2608 - acc: 0.9339 - val_loss: 0.2397 - val_acc: 0.9325\n",
      "Epoch 16/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2486 - acc: 0.9340 - val_loss: 0.2563 - val_acc: 0.9326\n",
      "Epoch 17/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2361 - acc: 0.9339 - val_loss: 0.2318 - val_acc: 0.9329\n",
      "Epoch 18/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2366 - acc: 0.9340 - val_loss: 0.2771 - val_acc: 0.9318\n",
      "Epoch 19/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2453 - acc: 0.9339 - val_loss: 0.2396 - val_acc: 0.9322\n",
      "Epoch 20/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2345 - acc: 0.9340 - val_loss: 0.2748 - val_acc: 0.9317\n",
      "Epoch 21/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2328 - acc: 0.9341 - val_loss: 0.2302 - val_acc: 0.9326\n",
      "Epoch 22/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2695 - acc: 0.9327 - val_loss: 0.2343 - val_acc: 0.9321\n",
      "Epoch 23/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.3027 - acc: 0.9295 - val_loss: 0.4564 - val_acc: 0.9322\n",
      "Epoch 24/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.3422 - acc: 0.9326 - val_loss: 0.3214 - val_acc: 0.9321\n",
      "Epoch 25/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2575 - acc: 0.9341 - val_loss: 0.2477 - val_acc: 0.9330\n",
      "Epoch 26/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2490 - acc: 0.9342 - val_loss: 0.2600 - val_acc: 0.9322\n",
      "Epoch 27/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2405 - acc: 0.9342 - val_loss: 0.2390 - val_acc: 0.9328\n",
      "Epoch 28/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2347 - acc: 0.9344 - val_loss: 0.2875 - val_acc: 0.9321\n",
      "Epoch 29/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2690 - acc: 0.9319 - val_loss: 0.2564 - val_acc: 0.9324\n",
      "Epoch 30/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2831 - acc: 0.9340 - val_loss: 0.2679 - val_acc: 0.9318\n",
      "Epoch 31/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.3649 - acc: 0.9137 - val_loss: 0.2526 - val_acc: 0.9311\n",
      "Epoch 32/100\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2425 - acc: 0.9340 - val_loss: 0.2363 - val_acc: 0.9328\n",
      "Epoch 33/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2372 - acc: 0.9343 - val_loss: 0.2374 - val_acc: 0.9318\n",
      "Epoch 34/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2337 - acc: 0.9341 - val_loss: 0.2457 - val_acc: 0.9324\n",
      "Epoch 35/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2340 - acc: 0.9342 - val_loss: 0.2309 - val_acc: 0.9323\n",
      "Epoch 36/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2274 - acc: 0.9345 - val_loss: 0.2354 - val_acc: 0.9323\n",
      "Epoch 37/100\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2492 - acc: 0.9343 - val_loss: 0.2554 - val_acc: 0.9320\n",
      "Epoch 38/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2402 - acc: 0.9344 - val_loss: 0.2302 - val_acc: 0.9329\n",
      "Epoch 39/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2218 - acc: 0.9347 - val_loss: 0.2278 - val_acc: 0.9321\n",
      "Epoch 40/100\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2190 - acc: 0.9344 - val_loss: 0.2241 - val_acc: 0.9326\n",
      "Epoch 41/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2269 - acc: 0.9347 - val_loss: 0.2417 - val_acc: 0.9333\n",
      "Epoch 42/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2296 - acc: 0.9348 - val_loss: 0.2417 - val_acc: 0.9332\n",
      "Epoch 43/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2128 - acc: 0.9357 - val_loss: 0.2024 - val_acc: 0.9349\n",
      "Epoch 44/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1979 - acc: 0.9362 - val_loss: 0.2017 - val_acc: 0.9349\n",
      "Epoch 45/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1998 - acc: 0.9361 - val_loss: 0.2047 - val_acc: 0.9346\n",
      "Epoch 46/100\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1992 - acc: 0.9363 - val_loss: 0.2070 - val_acc: 0.9344\n",
      "Epoch 47/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2027 - acc: 0.9363 - val_loss: 0.2084 - val_acc: 0.9341\n",
      "Epoch 48/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2178 - acc: 0.9360 - val_loss: 0.2246 - val_acc: 0.9347\n",
      "Epoch 49/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2100 - acc: 0.9363 - val_loss: 0.2085 - val_acc: 0.9345\n",
      "Epoch 50/100\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2021 - acc: 0.9363 - val_loss: 0.2098 - val_acc: 0.9345\n",
      "Epoch 51/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2054 - acc: 0.9362 - val_loss: 0.2247 - val_acc: 0.9336\n",
      "Epoch 52/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2371 - acc: 0.9275 - val_loss: 2.4781 - val_acc: 0.4665\n",
      "Epoch 53/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 2.1115 - acc: 0.7240 - val_loss: 1.0821 - val_acc: 0.9323\n",
      "Epoch 54/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.9476 - acc: 0.9192 - val_loss: 0.7013 - val_acc: 0.9269\n",
      "Epoch 55/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0306 - acc: 0.9331 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 56/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 58/100\n",
      "119999/119999 [==============================] - 8s 65us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 59/100\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 60/100\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 61/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 62/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 63/100\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 64/100\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 65/100\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 66/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 67/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 68/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 69/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 70/100\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 71/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 72/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 73/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 74/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 75/100\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 76/100\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 77/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 78/100\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 79/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 80/100\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 81/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 82/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 83/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 84/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 85/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 86/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 87/100\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 88/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 89/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 90/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 91/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 92/100\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 93/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 94/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 95/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 96/100\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 97/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 98/100\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 99/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 100/100\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/150\n",
      "119999/119999 [==============================] - 13s 109us/step - loss: 1.3840 - acc: 0.8752 - val_loss: 0.8432 - val_acc: 0.9148\n",
      "Epoch 2/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.7996 - acc: 0.9169 - val_loss: 0.8121 - val_acc: 0.9315\n",
      "Epoch 3/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.6121 - acc: 0.9121 - val_loss: 0.8220 - val_acc: 0.9304\n",
      "Epoch 4/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.7525 - acc: 0.9306 - val_loss: 0.6283 - val_acc: 0.9240\n",
      "Epoch 5/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.4292 - acc: 0.9263 - val_loss: 0.2987 - val_acc: 0.9303\n",
      "Epoch 6/150\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 1.1303 - acc: 0.8175 - val_loss: 0.7434 - val_acc: 0.9306\n",
      "Epoch 7/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.5386 - acc: 0.9229 - val_loss: 0.4955 - val_acc: 0.9308\n",
      "Epoch 8/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.7514 - acc: 0.8442 - val_loss: 2.8056 - val_acc: 0.4527\n",
      "Epoch 9/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 1.4553 - acc: 0.7588 - val_loss: 0.2946 - val_acc: 0.9303\n",
      "Epoch 10/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2822 - acc: 0.9331 - val_loss: 0.2507 - val_acc: 0.9314\n",
      "Epoch 11/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.3259 - acc: 0.9298 - val_loss: 0.2574 - val_acc: 0.9328\n",
      "Epoch 12/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2613 - acc: 0.9340 - val_loss: 0.2426 - val_acc: 0.9322\n",
      "Epoch 13/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.9671 - acc: 0.7775 - val_loss: 0.7748 - val_acc: 0.9302\n",
      "Epoch 14/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.5208 - acc: 0.9311 - val_loss: 0.2843 - val_acc: 0.9328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2950 - acc: 0.9337 - val_loss: 0.2359 - val_acc: 0.9323\n",
      "Epoch 16/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2339 - acc: 0.9337 - val_loss: 0.2354 - val_acc: 0.9320\n",
      "Epoch 17/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2376 - acc: 0.9342 - val_loss: 0.2649 - val_acc: 0.9323\n",
      "Epoch 18/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2503 - acc: 0.9340 - val_loss: 0.2847 - val_acc: 0.9325\n",
      "Epoch 19/150\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2565 - acc: 0.9339 - val_loss: 0.2864 - val_acc: 0.9320\n",
      "Epoch 20/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.3872 - acc: 0.8840 - val_loss: 0.6234 - val_acc: 0.7866\n",
      "Epoch 21/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.3907 - acc: 0.9192 - val_loss: 0.2596 - val_acc: 0.9329\n",
      "Epoch 22/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2609 - acc: 0.9341 - val_loss: 0.2524 - val_acc: 0.9318\n",
      "Epoch 23/150\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2348 - acc: 0.9342 - val_loss: 0.2319 - val_acc: 0.9327\n",
      "Epoch 24/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2274 - acc: 0.9341 - val_loss: 0.2426 - val_acc: 0.9334\n",
      "Epoch 25/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2291 - acc: 0.9339 - val_loss: 0.2336 - val_acc: 0.9328\n",
      "Epoch 26/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2367 - acc: 0.9340 - val_loss: 0.2478 - val_acc: 0.9324\n",
      "Epoch 27/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2360 - acc: 0.9341 - val_loss: 0.2348 - val_acc: 0.9323\n",
      "Epoch 28/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2489 - acc: 0.9339 - val_loss: 0.3013 - val_acc: 0.9325\n",
      "Epoch 29/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2413 - acc: 0.9342 - val_loss: 0.2333 - val_acc: 0.9321\n",
      "Epoch 30/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2536 - acc: 0.9340 - val_loss: 0.2302 - val_acc: 0.9321\n",
      "Epoch 31/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2239 - acc: 0.9343 - val_loss: 0.2252 - val_acc: 0.9327\n",
      "Epoch 32/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.4198 - acc: 0.8735 - val_loss: 0.3055 - val_acc: 0.9279\n",
      "Epoch 33/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2498 - acc: 0.9338 - val_loss: 0.2326 - val_acc: 0.9321\n",
      "Epoch 34/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2283 - acc: 0.9340 - val_loss: 0.2249 - val_acc: 0.9324\n",
      "Epoch 35/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2042 - acc: 0.9355 - val_loss: 0.2057 - val_acc: 0.9344\n",
      "Epoch 36/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2548 - acc: 0.9350 - val_loss: 0.2460 - val_acc: 0.9340\n",
      "Epoch 37/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2157 - acc: 0.9359 - val_loss: 0.2094 - val_acc: 0.9338\n",
      "Epoch 38/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2095 - acc: 0.9359 - val_loss: 0.2038 - val_acc: 0.9337\n",
      "Epoch 39/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2007 - acc: 0.9346 - val_loss: 0.2035 - val_acc: 0.9327\n",
      "Epoch 40/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.3279 - acc: 0.9022 - val_loss: 2.1342 - val_acc: 0.5024\n",
      "Epoch 41/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.9475 - acc: 0.7915 - val_loss: 0.2831 - val_acc: 0.9321\n",
      "Epoch 42/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2811 - acc: 0.9337 - val_loss: 0.2535 - val_acc: 0.9320\n",
      "Epoch 43/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2532 - acc: 0.9342 - val_loss: 0.3181 - val_acc: 0.9306\n",
      "Epoch 44/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.3142 - acc: 0.9293 - val_loss: 0.3410 - val_acc: 0.9314\n",
      "Epoch 45/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2474 - acc: 0.9342 - val_loss: 0.2115 - val_acc: 0.9341\n",
      "Epoch 46/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2045 - acc: 0.9358 - val_loss: 0.2093 - val_acc: 0.9349\n",
      "Epoch 47/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.3947 - acc: 0.8790 - val_loss: 0.3860 - val_acc: 0.9318\n",
      "Epoch 48/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2405 - acc: 0.9341 - val_loss: 0.2133 - val_acc: 0.9334\n",
      "Epoch 49/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2195 - acc: 0.9357 - val_loss: 0.2291 - val_acc: 0.9339\n",
      "Epoch 50/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2135 - acc: 0.9358 - val_loss: 0.2244 - val_acc: 0.9347\n",
      "Epoch 51/150\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2160 - acc: 0.9356 - val_loss: 0.2123 - val_acc: 0.9334\n",
      "Epoch 52/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2063 - acc: 0.9359 - val_loss: 0.2100 - val_acc: 0.9348\n",
      "Epoch 53/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2052 - acc: 0.9360 - val_loss: 0.2085 - val_acc: 0.9343\n",
      "Epoch 54/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2756 - acc: 0.9293 - val_loss: 0.2555 - val_acc: 0.9326\n",
      "Epoch 55/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2187 - acc: 0.9354 - val_loss: 0.2121 - val_acc: 0.9349\n",
      "Epoch 56/150\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2075 - acc: 0.9362 - val_loss: 0.2071 - val_acc: 0.9346\n",
      "Epoch 57/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2044 - acc: 0.9358 - val_loss: 0.2126 - val_acc: 0.9337\n",
      "Epoch 58/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2075 - acc: 0.9352 - val_loss: 0.2052 - val_acc: 0.9339\n",
      "Epoch 59/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2007 - acc: 0.9361 - val_loss: 0.2051 - val_acc: 0.9344\n",
      "Epoch 60/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1995 - acc: 0.9365 - val_loss: 0.2118 - val_acc: 0.9346\n",
      "Epoch 61/150\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2012 - acc: 0.9362 - val_loss: 0.2039 - val_acc: 0.9325\n",
      "Epoch 62/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2012 - acc: 0.9349 - val_loss: 0.2078 - val_acc: 0.9337\n",
      "Epoch 63/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1965 - acc: 0.9359 - val_loss: 0.1988 - val_acc: 0.9340\n",
      "Epoch 64/150\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2744 - acc: 0.9307 - val_loss: 0.2477 - val_acc: 0.9326\n",
      "Epoch 65/150\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2219 - acc: 0.9358 - val_loss: 0.2076 - val_acc: 0.9334\n",
      "Epoch 66/150\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.4416 - acc: 0.8640 - val_loss: 1.1135 - val_acc: 0.6537\n",
      "Epoch 67/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.4937 - acc: 0.8594 - val_loss: 0.5808 - val_acc: 0.8041\n",
      "Epoch 68/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.3193 - acc: 0.9097 - val_loss: 0.2761 - val_acc: 0.9308\n",
      "Epoch 69/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2851 - acc: 0.9295 - val_loss: 0.2635 - val_acc: 0.9334\n",
      "Epoch 70/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2376 - acc: 0.9359 - val_loss: 0.2197 - val_acc: 0.9350\n",
      "Epoch 71/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2256 - acc: 0.9362 - val_loss: 0.2221 - val_acc: 0.9341\n",
      "Epoch 72/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2098 - acc: 0.9362 - val_loss: 0.2082 - val_acc: 0.9341\n",
      "Epoch 73/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2014 - acc: 0.9360 - val_loss: 0.2045 - val_acc: 0.9328\n",
      "Epoch 74/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2014 - acc: 0.9355 - val_loss: 0.2002 - val_acc: 0.9342\n",
      "Epoch 75/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1964 - acc: 0.9364 - val_loss: 0.2006 - val_acc: 0.9350\n",
      "Epoch 76/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1947 - acc: 0.9363 - val_loss: 0.1992 - val_acc: 0.9349\n",
      "Epoch 77/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1961 - acc: 0.9361 - val_loss: 0.1998 - val_acc: 0.9349\n",
      "Epoch 78/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1940 - acc: 0.9365 - val_loss: 0.1980 - val_acc: 0.9351\n",
      "Epoch 79/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1941 - acc: 0.9363 - val_loss: 0.1979 - val_acc: 0.9352\n",
      "Epoch 80/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1939 - acc: 0.9365 - val_loss: 0.1998 - val_acc: 0.9346\n",
      "Epoch 81/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1999 - acc: 0.9366 - val_loss: 0.2017 - val_acc: 0.9350\n",
      "Epoch 82/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1963 - acc: 0.9366 - val_loss: 0.2009 - val_acc: 0.9351\n",
      "Epoch 83/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2074 - acc: 0.9367 - val_loss: 0.2228 - val_acc: 0.9342\n",
      "Epoch 84/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2051 - acc: 0.9362 - val_loss: 0.2052 - val_acc: 0.9349\n",
      "Epoch 85/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1956 - acc: 0.9367 - val_loss: 0.1982 - val_acc: 0.9354\n",
      "Epoch 86/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1977 - acc: 0.9366 - val_loss: 0.2292 - val_acc: 0.9336\n",
      "Epoch 87/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2169 - acc: 0.9349 - val_loss: 0.2087 - val_acc: 0.9335\n",
      "Epoch 88/150\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1983 - acc: 0.9364 - val_loss: 0.1984 - val_acc: 0.9343\n",
      "Epoch 89/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1979 - acc: 0.9363 - val_loss: 0.2041 - val_acc: 0.9343\n",
      "Epoch 90/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1971 - acc: 0.9364 - val_loss: 0.1988 - val_acc: 0.9353\n",
      "Epoch 91/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1953 - acc: 0.9365 - val_loss: 0.1978 - val_acc: 0.9350\n",
      "Epoch 92/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1956 - acc: 0.9366 - val_loss: 0.1978 - val_acc: 0.9349\n",
      "Epoch 93/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1929 - acc: 0.9366 - val_loss: 0.1966 - val_acc: 0.9350\n",
      "Epoch 94/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1917 - acc: 0.9368 - val_loss: 0.1966 - val_acc: 0.9351\n",
      "Epoch 95/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1914 - acc: 0.9368 - val_loss: 0.1962 - val_acc: 0.9349\n",
      "Epoch 96/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1911 - acc: 0.9368 - val_loss: 0.1968 - val_acc: 0.9347\n",
      "Epoch 97/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1914 - acc: 0.9367 - val_loss: 0.1968 - val_acc: 0.9350\n",
      "Epoch 98/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1929 - acc: 0.9368 - val_loss: 0.1977 - val_acc: 0.9352\n",
      "Epoch 99/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1915 - acc: 0.9369 - val_loss: 0.1972 - val_acc: 0.9349\n",
      "Epoch 100/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1909 - acc: 0.9371 - val_loss: 0.1967 - val_acc: 0.9351\n",
      "Epoch 101/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1903 - acc: 0.9371 - val_loss: 0.1969 - val_acc: 0.9348\n",
      "Epoch 102/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1902 - acc: 0.9371 - val_loss: 0.1966 - val_acc: 0.9348\n",
      "Epoch 103/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1905 - acc: 0.9370 - val_loss: 0.1976 - val_acc: 0.9348\n",
      "Epoch 104/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1953 - acc: 0.9370 - val_loss: 0.2111 - val_acc: 0.9344\n",
      "Epoch 105/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1971 - acc: 0.9367 - val_loss: 0.1970 - val_acc: 0.9353\n",
      "Epoch 106/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1942 - acc: 0.9371 - val_loss: 0.2072 - val_acc: 0.9337\n",
      "Epoch 107/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1974 - acc: 0.9364 - val_loss: 0.1990 - val_acc: 0.9350\n",
      "Epoch 108/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1914 - acc: 0.9369 - val_loss: 0.1959 - val_acc: 0.9346\n",
      "Epoch 109/150\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1903 - acc: 0.9372 - val_loss: 0.1962 - val_acc: 0.9347\n",
      "Epoch 110/150\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1901 - acc: 0.9372 - val_loss: 0.1974 - val_acc: 0.9347\n",
      "Epoch 111/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1900 - acc: 0.9372 - val_loss: 0.1963 - val_acc: 0.9348\n",
      "Epoch 112/150\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1898 - acc: 0.9372 - val_loss: 0.1967 - val_acc: 0.9348\n",
      "Epoch 113/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1894 - acc: 0.9374 - val_loss: 0.1969 - val_acc: 0.9348\n",
      "Epoch 114/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1895 - acc: 0.9374 - val_loss: 0.1979 - val_acc: 0.9344\n",
      "Epoch 115/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1904 - acc: 0.9374 - val_loss: 0.1973 - val_acc: 0.9349\n",
      "Epoch 116/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1896 - acc: 0.9374 - val_loss: 0.1987 - val_acc: 0.9347\n",
      "Epoch 117/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1894 - acc: 0.9375 - val_loss: 0.1968 - val_acc: 0.9352\n",
      "Epoch 118/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1896 - acc: 0.9375 - val_loss: 0.1983 - val_acc: 0.9351\n",
      "Epoch 119/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1892 - acc: 0.9373 - val_loss: 0.1977 - val_acc: 0.9348\n",
      "Epoch 120/150\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1891 - acc: 0.9375 - val_loss: 0.1990 - val_acc: 0.9353\n",
      "Epoch 121/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1888 - acc: 0.9377 - val_loss: 0.1976 - val_acc: 0.9350\n",
      "Epoch 122/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1888 - acc: 0.9378 - val_loss: 0.2019 - val_acc: 0.9351\n",
      "Epoch 123/150\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1883 - acc: 0.9378 - val_loss: 0.1981 - val_acc: 0.9354\n",
      "Epoch 124/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1882 - acc: 0.9375 - val_loss: 0.1972 - val_acc: 0.9351\n",
      "Epoch 125/150\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1880 - acc: 0.9377 - val_loss: 0.1992 - val_acc: 0.9352\n",
      "Epoch 126/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1880 - acc: 0.9377 - val_loss: 0.1979 - val_acc: 0.9352\n",
      "Epoch 127/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1877 - acc: 0.9378 - val_loss: 0.1979 - val_acc: 0.9352\n",
      "Epoch 128/150\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1876 - acc: 0.9379 - val_loss: 0.1992 - val_acc: 0.9354\n",
      "Epoch 129/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1953 - acc: 0.9371 - val_loss: 0.2129 - val_acc: 0.9336\n",
      "Epoch 130/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1969 - acc: 0.9374 - val_loss: 0.2008 - val_acc: 0.9347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/150\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1915 - acc: 0.9377 - val_loss: 0.1991 - val_acc: 0.9356\n",
      "Epoch 132/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1901 - acc: 0.9377 - val_loss: 0.2019 - val_acc: 0.9347\n",
      "Epoch 133/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1888 - acc: 0.9380 - val_loss: 0.2005 - val_acc: 0.9345\n",
      "Epoch 134/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1888 - acc: 0.9378 - val_loss: 0.1995 - val_acc: 0.9354\n",
      "Epoch 135/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1884 - acc: 0.9380 - val_loss: 0.2017 - val_acc: 0.9350\n",
      "Epoch 136/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1877 - acc: 0.9381 - val_loss: 0.2011 - val_acc: 0.9350\n",
      "Epoch 137/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1875 - acc: 0.9382 - val_loss: 0.2004 - val_acc: 0.9352\n",
      "Epoch 138/150\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1874 - acc: 0.9381 - val_loss: 0.1975 - val_acc: 0.9348\n",
      "Epoch 139/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1878 - acc: 0.9382 - val_loss: 0.2020 - val_acc: 0.9351\n",
      "Epoch 140/150\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2139 - acc: 0.9379 - val_loss: 0.2326 - val_acc: 0.9332\n",
      "Epoch 141/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2098 - acc: 0.9372 - val_loss: 0.2139 - val_acc: 0.9362\n",
      "Epoch 142/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.3632 - acc: 0.8891 - val_loss: 0.3610 - val_acc: 0.9323\n",
      "Epoch 143/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2395 - acc: 0.9364 - val_loss: 0.2279 - val_acc: 0.9347\n",
      "Epoch 144/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2074 - acc: 0.9374 - val_loss: 0.2173 - val_acc: 0.9356\n",
      "Epoch 145/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2087 - acc: 0.9375 - val_loss: 0.2204 - val_acc: 0.9349\n",
      "Epoch 146/150\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2029 - acc: 0.9382 - val_loss: 0.2089 - val_acc: 0.9346\n",
      "Epoch 147/150\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1967 - acc: 0.9381 - val_loss: 0.2080 - val_acc: 0.9354\n",
      "Epoch 148/150\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1948 - acc: 0.9382 - val_loss: 0.2065 - val_acc: 0.9351\n",
      "Epoch 149/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2041 - acc: 0.9383 - val_loss: 0.2439 - val_acc: 0.9350\n",
      "Epoch 150/150\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2102 - acc: 0.9373 - val_loss: 0.2101 - val_acc: 0.9335\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/200\n",
      "119999/119999 [==============================] - 14s 118us/step - loss: 0.7994 - acc: 0.9147 - val_loss: 0.9196 - val_acc: 0.9299\n",
      "Epoch 2/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.7598 - acc: 0.9274 - val_loss: 0.5972 - val_acc: 0.9315\n",
      "Epoch 3/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.4615 - acc: 0.9289 - val_loss: 0.5233 - val_acc: 0.8724\n",
      "Epoch 4/200\n",
      "119999/119999 [==============================] - 8s 65us/step - loss: 0.4292 - acc: 0.9216 - val_loss: 0.3216 - val_acc: 0.9222\n",
      "Epoch 5/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.3766 - acc: 0.9267 - val_loss: 0.3924 - val_acc: 0.9325\n",
      "Epoch 6/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.4370 - acc: 0.9019 - val_loss: 0.8590 - val_acc: 0.9302\n",
      "Epoch 7/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.7376 - acc: 0.9254 - val_loss: 0.9694 - val_acc: 0.9301\n",
      "Epoch 8/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.9435 - acc: 0.9310 - val_loss: 0.8932 - val_acc: 0.9286\n",
      "Epoch 9/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.7668 - acc: 0.9281 - val_loss: 0.5443 - val_acc: 0.8864\n",
      "Epoch 10/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.5840 - acc: 0.9235 - val_loss: 0.4245 - val_acc: 0.9230\n",
      "Epoch 11/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3458 - acc: 0.9315 - val_loss: 0.3670 - val_acc: 0.9310\n",
      "Epoch 12/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3631 - acc: 0.9325 - val_loss: 0.4579 - val_acc: 0.9314\n",
      "Epoch 13/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3739 - acc: 0.9334 - val_loss: 0.2804 - val_acc: 0.9305\n",
      "Epoch 14/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.3101 - acc: 0.9313 - val_loss: 0.2689 - val_acc: 0.9320\n",
      "Epoch 15/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.5044 - acc: 0.9088 - val_loss: 0.5980 - val_acc: 0.9318\n",
      "Epoch 16/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.4258 - acc: 0.9315 - val_loss: 0.3103 - val_acc: 0.9278\n",
      "Epoch 17/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3697 - acc: 0.9075 - val_loss: 0.2844 - val_acc: 0.9316\n",
      "Epoch 18/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2770 - acc: 0.9334 - val_loss: 0.2756 - val_acc: 0.9315\n",
      "Epoch 19/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2845 - acc: 0.9334 - val_loss: 0.2569 - val_acc: 0.9322\n",
      "Epoch 20/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2514 - acc: 0.9340 - val_loss: 0.2445 - val_acc: 0.9331\n",
      "Epoch 21/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2344 - acc: 0.9342 - val_loss: 0.2369 - val_acc: 0.9329\n",
      "Epoch 22/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2528 - acc: 0.9338 - val_loss: 0.2740 - val_acc: 0.9325\n",
      "Epoch 23/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2555 - acc: 0.9339 - val_loss: 0.2359 - val_acc: 0.9328\n",
      "Epoch 24/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2248 - acc: 0.9342 - val_loss: 0.2256 - val_acc: 0.9331\n",
      "Epoch 25/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2236 - acc: 0.9342 - val_loss: 0.2247 - val_acc: 0.9329\n",
      "Epoch 26/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2286 - acc: 0.9341 - val_loss: 0.2363 - val_acc: 0.9323\n",
      "Epoch 27/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.2298 - acc: 0.9340 - val_loss: 0.2367 - val_acc: 0.9335\n",
      "Epoch 28/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2269 - acc: 0.9341 - val_loss: 0.2298 - val_acc: 0.9333\n",
      "Epoch 29/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2225 - acc: 0.9340 - val_loss: 0.2259 - val_acc: 0.9326\n",
      "Epoch 30/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2233 - acc: 0.9342 - val_loss: 0.2250 - val_acc: 0.9328\n",
      "Epoch 31/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2274 - acc: 0.9343 - val_loss: 0.2248 - val_acc: 0.9324\n",
      "Epoch 32/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2202 - acc: 0.9344 - val_loss: 0.2266 - val_acc: 0.9329\n",
      "Epoch 33/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.3608 - acc: 0.8971 - val_loss: 0.6663 - val_acc: 0.9309\n",
      "Epoch 34/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.3920 - acc: 0.9182 - val_loss: 0.4072 - val_acc: 0.9071\n",
      "Epoch 35/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.3263 - acc: 0.9309 - val_loss: 0.2723 - val_acc: 0.9308\n",
      "Epoch 36/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2620 - acc: 0.9331 - val_loss: 0.2482 - val_acc: 0.9320\n",
      "Epoch 37/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2425 - acc: 0.9332 - val_loss: 0.2334 - val_acc: 0.9318\n",
      "Epoch 38/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2257 - acc: 0.9342 - val_loss: 0.2243 - val_acc: 0.9327\n",
      "Epoch 39/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2218 - acc: 0.9339 - val_loss: 0.2219 - val_acc: 0.9330\n",
      "Epoch 40/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2190 - acc: 0.9342 - val_loss: 0.2201 - val_acc: 0.9328\n",
      "Epoch 41/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2192 - acc: 0.9342 - val_loss: 0.2286 - val_acc: 0.9329\n",
      "Epoch 42/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2216 - acc: 0.9342 - val_loss: 0.2215 - val_acc: 0.9331\n",
      "Epoch 43/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2196 - acc: 0.9346 - val_loss: 0.2344 - val_acc: 0.9327\n",
      "Epoch 44/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2204 - acc: 0.9344 - val_loss: 0.2340 - val_acc: 0.9334\n",
      "Epoch 45/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2999 - acc: 0.9197 - val_loss: 0.4135 - val_acc: 0.9076\n",
      "Epoch 46/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.3062 - acc: 0.9332 - val_loss: 0.2311 - val_acc: 0.9348\n",
      "Epoch 47/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2338 - acc: 0.9363 - val_loss: 0.2698 - val_acc: 0.9348\n",
      "Epoch 48/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2487 - acc: 0.9344 - val_loss: 0.3595 - val_acc: 0.9221\n",
      "Epoch 49/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2852 - acc: 0.9329 - val_loss: 0.2234 - val_acc: 0.9339\n",
      "Epoch 50/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2201 - acc: 0.9356 - val_loss: 0.2175 - val_acc: 0.9338\n",
      "Epoch 51/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2086 - acc: 0.9355 - val_loss: 0.2038 - val_acc: 0.9335\n",
      "Epoch 52/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.2004 - acc: 0.9347 - val_loss: 0.2033 - val_acc: 0.9338\n",
      "Epoch 53/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.1970 - acc: 0.9357 - val_loss: 0.1986 - val_acc: 0.9344\n",
      "Epoch 54/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1940 - acc: 0.9359 - val_loss: 0.1972 - val_acc: 0.9344\n",
      "Epoch 55/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1938 - acc: 0.9358 - val_loss: 0.1979 - val_acc: 0.9344\n",
      "Epoch 56/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1944 - acc: 0.9361 - val_loss: 0.1973 - val_acc: 0.9346\n",
      "Epoch 57/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1932 - acc: 0.9360 - val_loss: 0.1970 - val_acc: 0.9346\n",
      "Epoch 58/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1925 - acc: 0.9361 - val_loss: 0.1975 - val_acc: 0.9347\n",
      "Epoch 59/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1926 - acc: 0.9361 - val_loss: 0.1977 - val_acc: 0.9343\n",
      "Epoch 60/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1929 - acc: 0.9360 - val_loss: 0.1977 - val_acc: 0.9345\n",
      "Epoch 61/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.3369 - acc: 0.8972 - val_loss: 0.3207 - val_acc: 0.9315\n",
      "Epoch 62/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.3957 - acc: 0.9337 - val_loss: 0.2824 - val_acc: 0.9343\n",
      "Epoch 63/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2444 - acc: 0.9356 - val_loss: 0.2207 - val_acc: 0.9338\n",
      "Epoch 64/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2179 - acc: 0.9356 - val_loss: 0.2235 - val_acc: 0.9343\n",
      "Epoch 65/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2177 - acc: 0.9347 - val_loss: 0.2269 - val_acc: 0.9330\n",
      "Epoch 66/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2144 - acc: 0.9356 - val_loss: 0.2160 - val_acc: 0.9340\n",
      "Epoch 67/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2035 - acc: 0.9360 - val_loss: 0.2036 - val_acc: 0.9349\n",
      "Epoch 68/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1999 - acc: 0.9362 - val_loss: 0.1997 - val_acc: 0.9346\n",
      "Epoch 69/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1960 - acc: 0.9362 - val_loss: 0.1996 - val_acc: 0.9346\n",
      "Epoch 70/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1948 - acc: 0.9362 - val_loss: 0.1976 - val_acc: 0.9349\n",
      "Epoch 71/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1941 - acc: 0.9363 - val_loss: 0.1991 - val_acc: 0.9345\n",
      "Epoch 72/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1943 - acc: 0.9363 - val_loss: 0.1977 - val_acc: 0.9344\n",
      "Epoch 73/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1928 - acc: 0.9362 - val_loss: 0.1976 - val_acc: 0.9344\n",
      "Epoch 74/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1998 - acc: 0.9364 - val_loss: 0.2196 - val_acc: 0.9343\n",
      "Epoch 75/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2053 - acc: 0.9364 - val_loss: 0.2032 - val_acc: 0.9355\n",
      "Epoch 76/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1966 - acc: 0.9365 - val_loss: 0.2005 - val_acc: 0.9347\n",
      "Epoch 77/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1953 - acc: 0.9365 - val_loss: 0.1984 - val_acc: 0.9349\n",
      "Epoch 78/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1943 - acc: 0.9364 - val_loss: 0.1971 - val_acc: 0.9343\n",
      "Epoch 79/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1923 - acc: 0.9362 - val_loss: 0.1974 - val_acc: 0.9344\n",
      "Epoch 80/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2000 - acc: 0.9364 - val_loss: 0.2025 - val_acc: 0.9341\n",
      "Epoch 81/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1965 - acc: 0.9366 - val_loss: 0.2001 - val_acc: 0.9345\n",
      "Epoch 82/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1955 - acc: 0.9366 - val_loss: 0.1972 - val_acc: 0.9350\n",
      "Epoch 83/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1922 - acc: 0.9366 - val_loss: 0.1983 - val_acc: 0.9346\n",
      "Epoch 84/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1915 - acc: 0.9366 - val_loss: 0.1958 - val_acc: 0.9341\n",
      "Epoch 85/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1910 - acc: 0.9367 - val_loss: 0.1970 - val_acc: 0.9350\n",
      "Epoch 86/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1910 - acc: 0.9368 - val_loss: 0.1967 - val_acc: 0.9348\n",
      "Epoch 87/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1915 - acc: 0.9368 - val_loss: 0.1966 - val_acc: 0.9348\n",
      "Epoch 88/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1913 - acc: 0.9368 - val_loss: 0.1964 - val_acc: 0.9349\n",
      "Epoch 89/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1906 - acc: 0.9369 - val_loss: 0.1981 - val_acc: 0.9351\n",
      "Epoch 90/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1932 - acc: 0.9368 - val_loss: 0.2026 - val_acc: 0.9346\n",
      "Epoch 91/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1912 - acc: 0.9370 - val_loss: 0.1974 - val_acc: 0.9347\n",
      "Epoch 92/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1901 - acc: 0.9368 - val_loss: 0.1988 - val_acc: 0.9351\n",
      "Epoch 93/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1899 - acc: 0.9368 - val_loss: 0.1984 - val_acc: 0.9350\n",
      "Epoch 94/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1899 - acc: 0.9370 - val_loss: 0.1981 - val_acc: 0.9352\n",
      "Epoch 95/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1898 - acc: 0.9369 - val_loss: 0.1980 - val_acc: 0.9350\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1896 - acc: 0.9370 - val_loss: 0.1986 - val_acc: 0.9349\n",
      "Epoch 97/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1896 - acc: 0.9371 - val_loss: 0.1994 - val_acc: 0.9347\n",
      "Epoch 98/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1898 - acc: 0.9370 - val_loss: 0.2013 - val_acc: 0.9346\n",
      "Epoch 99/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1898 - acc: 0.9371 - val_loss: 0.1997 - val_acc: 0.9343\n",
      "Epoch 100/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1897 - acc: 0.9371 - val_loss: 0.1980 - val_acc: 0.9345\n",
      "Epoch 101/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1899 - acc: 0.9369 - val_loss: 0.1990 - val_acc: 0.9348\n",
      "Epoch 102/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1905 - acc: 0.9364 - val_loss: 0.2000 - val_acc: 0.9338\n",
      "Epoch 103/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1899 - acc: 0.9370 - val_loss: 0.1976 - val_acc: 0.9347\n",
      "Epoch 104/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1893 - acc: 0.9370 - val_loss: 0.1979 - val_acc: 0.9342\n",
      "Epoch 105/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1892 - acc: 0.9372 - val_loss: 0.1983 - val_acc: 0.9348\n",
      "Epoch 106/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1924 - acc: 0.9371 - val_loss: 0.1979 - val_acc: 0.9347\n",
      "Epoch 107/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1918 - acc: 0.9370 - val_loss: 0.1985 - val_acc: 0.9346\n",
      "Epoch 108/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1899 - acc: 0.9372 - val_loss: 0.1977 - val_acc: 0.9347\n",
      "Epoch 109/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1893 - acc: 0.9373 - val_loss: 0.1986 - val_acc: 0.9345\n",
      "Epoch 110/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1891 - acc: 0.9372 - val_loss: 0.1984 - val_acc: 0.9347\n",
      "Epoch 111/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1890 - acc: 0.9374 - val_loss: 0.1992 - val_acc: 0.9347\n",
      "Epoch 112/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1889 - acc: 0.9373 - val_loss: 0.2001 - val_acc: 0.9345\n",
      "Epoch 113/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1886 - acc: 0.9375 - val_loss: 0.1996 - val_acc: 0.9347\n",
      "Epoch 114/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1887 - acc: 0.9374 - val_loss: 0.1990 - val_acc: 0.9346\n",
      "Epoch 115/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1885 - acc: 0.9377 - val_loss: 0.1995 - val_acc: 0.9342\n",
      "Epoch 116/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1886 - acc: 0.9373 - val_loss: 0.1988 - val_acc: 0.9345\n",
      "Epoch 117/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1885 - acc: 0.9375 - val_loss: 0.2009 - val_acc: 0.9344\n",
      "Epoch 118/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1883 - acc: 0.9375 - val_loss: 0.1996 - val_acc: 0.9344\n",
      "Epoch 119/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1882 - acc: 0.9376 - val_loss: 0.1999 - val_acc: 0.9344\n",
      "Epoch 120/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1879 - acc: 0.9377 - val_loss: 0.1991 - val_acc: 0.9346\n",
      "Epoch 121/200\n",
      "119999/119999 [==============================] - 10s 80us/step - loss: 0.1878 - acc: 0.9376 - val_loss: 0.2003 - val_acc: 0.9344\n",
      "Epoch 122/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1878 - acc: 0.9376 - val_loss: 0.2019 - val_acc: 0.9345\n",
      "Epoch 123/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1884 - acc: 0.9377 - val_loss: 0.2009 - val_acc: 0.9343\n",
      "Epoch 124/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1885 - acc: 0.9375 - val_loss: 0.2038 - val_acc: 0.9344\n",
      "Epoch 125/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.1896 - acc: 0.9376 - val_loss: 0.2006 - val_acc: 0.9345\n",
      "Epoch 126/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.1881 - acc: 0.9375 - val_loss: 0.2005 - val_acc: 0.9344\n",
      "Epoch 127/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1875 - acc: 0.9378 - val_loss: 0.2019 - val_acc: 0.9345\n",
      "Epoch 128/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1873 - acc: 0.9379 - val_loss: 0.2007 - val_acc: 0.9345\n",
      "Epoch 129/200\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 0.1876 - acc: 0.9377 - val_loss: 0.2035 - val_acc: 0.9342\n",
      "Epoch 130/200\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1874 - acc: 0.9378 - val_loss: 0.2015 - val_acc: 0.9343\n",
      "Epoch 131/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1870 - acc: 0.9379 - val_loss: 0.2022 - val_acc: 0.9341\n",
      "Epoch 132/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1896 - acc: 0.9378 - val_loss: 0.2046 - val_acc: 0.9348\n",
      "Epoch 133/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1911 - acc: 0.9376 - val_loss: 0.2029 - val_acc: 0.9349\n",
      "Epoch 134/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1906 - acc: 0.9375 - val_loss: 0.2037 - val_acc: 0.9344\n",
      "Epoch 135/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1899 - acc: 0.9378 - val_loss: 0.2033 - val_acc: 0.9346\n",
      "Epoch 136/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1888 - acc: 0.9381 - val_loss: 0.2023 - val_acc: 0.9340\n",
      "Epoch 137/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1878 - acc: 0.9381 - val_loss: 0.2015 - val_acc: 0.9347\n",
      "Epoch 138/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1873 - acc: 0.9381 - val_loss: 0.2041 - val_acc: 0.9344\n",
      "Epoch 139/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1875 - acc: 0.9382 - val_loss: 0.2016 - val_acc: 0.9346\n",
      "Epoch 140/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1872 - acc: 0.9382 - val_loss: 0.2024 - val_acc: 0.9345\n",
      "Epoch 141/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1869 - acc: 0.9382 - val_loss: 0.2019 - val_acc: 0.9343\n",
      "Epoch 142/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1874 - acc: 0.9382 - val_loss: 0.2058 - val_acc: 0.9344\n",
      "Epoch 143/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1892 - acc: 0.9383 - val_loss: 0.2055 - val_acc: 0.9348\n",
      "Epoch 144/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1884 - acc: 0.9380 - val_loss: 0.2029 - val_acc: 0.9348\n",
      "Epoch 145/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1870 - acc: 0.9381 - val_loss: 0.2034 - val_acc: 0.9344\n",
      "Epoch 146/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1868 - acc: 0.9384 - val_loss: 0.2046 - val_acc: 0.9344\n",
      "Epoch 147/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1864 - acc: 0.9382 - val_loss: 0.2063 - val_acc: 0.9342\n",
      "Epoch 148/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1863 - acc: 0.9382 - val_loss: 0.2059 - val_acc: 0.9345\n",
      "Epoch 149/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1870 - acc: 0.9383 - val_loss: 0.2050 - val_acc: 0.9346\n",
      "Epoch 150/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1870 - acc: 0.9380 - val_loss: 0.2038 - val_acc: 0.9346\n",
      "Epoch 151/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1864 - acc: 0.9385 - val_loss: 0.2051 - val_acc: 0.9343\n",
      "Epoch 152/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1859 - acc: 0.9388 - val_loss: 0.2043 - val_acc: 0.9348\n",
      "Epoch 153/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1858 - acc: 0.9386 - val_loss: 0.2058 - val_acc: 0.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1857 - acc: 0.9387 - val_loss: 0.2057 - val_acc: 0.9352\n",
      "Epoch 155/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1858 - acc: 0.9387 - val_loss: 0.2069 - val_acc: 0.9338\n",
      "Epoch 156/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1855 - acc: 0.9386 - val_loss: 0.2076 - val_acc: 0.9344\n",
      "Epoch 157/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1854 - acc: 0.9387 - val_loss: 0.2054 - val_acc: 0.9349\n",
      "Epoch 158/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1856 - acc: 0.9389 - val_loss: 0.2077 - val_acc: 0.9342\n",
      "Epoch 159/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1867 - acc: 0.9386 - val_loss: 0.2064 - val_acc: 0.9347\n",
      "Epoch 160/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1859 - acc: 0.9388 - val_loss: 0.2038 - val_acc: 0.9341\n",
      "Epoch 161/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1858 - acc: 0.9387 - val_loss: 0.2061 - val_acc: 0.9341\n",
      "Epoch 162/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1858 - acc: 0.9386 - val_loss: 0.2060 - val_acc: 0.9336\n",
      "Epoch 163/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1939 - acc: 0.9371 - val_loss: 0.2299 - val_acc: 0.9337\n",
      "Epoch 164/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1959 - acc: 0.9374 - val_loss: 0.2071 - val_acc: 0.9350\n",
      "Epoch 165/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1905 - acc: 0.9376 - val_loss: 0.2063 - val_acc: 0.9359\n",
      "Epoch 166/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1901 - acc: 0.9384 - val_loss: 0.2056 - val_acc: 0.9353\n",
      "Epoch 167/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1888 - acc: 0.9386 - val_loss: 0.2070 - val_acc: 0.9359\n",
      "Epoch 168/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1888 - acc: 0.9386 - val_loss: 0.2072 - val_acc: 0.9355\n",
      "Epoch 169/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1888 - acc: 0.9389 - val_loss: 0.2086 - val_acc: 0.9349\n",
      "Epoch 170/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1884 - acc: 0.9391 - val_loss: 0.2084 - val_acc: 0.9358\n",
      "Epoch 171/200\n",
      "119999/119999 [==============================] - 8s 66us/step - loss: 0.1881 - acc: 0.9391 - val_loss: 0.2082 - val_acc: 0.9354\n",
      "Epoch 172/200\n",
      "119999/119999 [==============================] - 9s 72us/step - loss: 0.1878 - acc: 0.9391 - val_loss: 0.2086 - val_acc: 0.9351\n",
      "Epoch 173/200\n",
      "119999/119999 [==============================] - 8s 67us/step - loss: 0.1875 - acc: 0.9393 - val_loss: 0.2111 - val_acc: 0.9352\n",
      "Epoch 174/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1873 - acc: 0.9394 - val_loss: 0.2098 - val_acc: 0.9354\n",
      "Epoch 175/200\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1888 - acc: 0.9393 - val_loss: 0.2148 - val_acc: 0.9342\n",
      "Epoch 176/200\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1921 - acc: 0.9388 - val_loss: 0.2123 - val_acc: 0.9350\n",
      "Epoch 177/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1883 - acc: 0.9394 - val_loss: 0.2102 - val_acc: 0.9352\n",
      "Epoch 178/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1875 - acc: 0.9395 - val_loss: 0.2105 - val_acc: 0.9350\n",
      "Epoch 179/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1877 - acc: 0.9393 - val_loss: 0.2119 - val_acc: 0.9354\n",
      "Epoch 180/200\n",
      "119999/119999 [==============================] - 8s 68us/step - loss: 0.1875 - acc: 0.9393 - val_loss: 0.2123 - val_acc: 0.9349\n",
      "Epoch 181/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1875 - acc: 0.9397 - val_loss: 0.2111 - val_acc: 0.9346\n",
      "Epoch 182/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1875 - acc: 0.9396 - val_loss: 0.2161 - val_acc: 0.9349\n",
      "Epoch 183/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1903 - acc: 0.9392 - val_loss: 0.2139 - val_acc: 0.9354\n",
      "Epoch 184/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1878 - acc: 0.9396 - val_loss: 0.2124 - val_acc: 0.9350\n",
      "Epoch 185/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1870 - acc: 0.9396 - val_loss: 0.2110 - val_acc: 0.9351\n",
      "Epoch 186/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1868 - acc: 0.9398 - val_loss: 0.2136 - val_acc: 0.9350\n",
      "Epoch 187/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1867 - acc: 0.9397 - val_loss: 0.2148 - val_acc: 0.9353\n",
      "Epoch 188/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1952 - acc: 0.9392 - val_loss: 0.2267 - val_acc: 0.9344\n",
      "Epoch 189/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1909 - acc: 0.9391 - val_loss: 0.2133 - val_acc: 0.9355\n",
      "Epoch 190/200\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1870 - acc: 0.9399 - val_loss: 0.2111 - val_acc: 0.9347\n",
      "Epoch 191/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1872 - acc: 0.9400 - val_loss: 0.2114 - val_acc: 0.9351\n",
      "Epoch 192/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1868 - acc: 0.9400 - val_loss: 0.2111 - val_acc: 0.9350\n",
      "Epoch 193/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1864 - acc: 0.9398 - val_loss: 0.2154 - val_acc: 0.9348\n",
      "Epoch 194/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1867 - acc: 0.9396 - val_loss: 0.2133 - val_acc: 0.9353\n",
      "Epoch 195/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1878 - acc: 0.9398 - val_loss: 0.2197 - val_acc: 0.9346\n",
      "Epoch 196/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1871 - acc: 0.9401 - val_loss: 0.2176 - val_acc: 0.9348\n",
      "Epoch 197/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1861 - acc: 0.9399 - val_loss: 0.2158 - val_acc: 0.9343\n",
      "Epoch 198/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1859 - acc: 0.9404 - val_loss: 0.2147 - val_acc: 0.9350\n",
      "Epoch 199/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1935 - acc: 0.9400 - val_loss: 0.2262 - val_acc: 0.9347\n",
      "Epoch 200/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2015 - acc: 0.9388 - val_loss: 0.2211 - val_acc: 0.9344\n"
     ]
    }
   ],
   "source": [
    "num_epochs = [10, 50, 100, 150, 200]\n",
    "scores = []\n",
    "for i in range(len(num_epochs)):\n",
    "    epoch_model = Sequential()\n",
    "    epoch_model.add(Dense(units = 600, input_dim = 10, activation = 'relu'))\n",
    "    epoch_model.add(Dense(units = 300, activation = 'relu'))\n",
    "    epoch_model.add(Dense(units = 100, activation = 'relu'))\n",
    "    epoch_model.add(Dense(units = 1, activation = 'relu'))\n",
    "\n",
    "    epoch_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    epoch_log = epoch_model.fit(X_train, Y_train, batch_size = 6000,\n",
    "                    epochs= num_epochs[i], verbose = 1,\n",
    "                    validation_data = (X_validation, Y_validation))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best Epoch = 190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/190\n",
      "119999/119999 [==============================] - 11s 94us/step - loss: 0.7857 - acc: 0.9107 - val_loss: 0.7910 - val_acc: 0.9310\n",
      "Epoch 2/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.5947 - acc: 0.9234 - val_loss: 0.5704 - val_acc: 0.9320\n",
      "Epoch 3/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.6529 - acc: 0.9057 - val_loss: 0.8388 - val_acc: 0.9301\n",
      "Epoch 4/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.6431 - acc: 0.9126 - val_loss: 1.0238 - val_acc: 0.9302\n",
      "Epoch 5/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0432 - acc: 0.9319 - val_loss: 1.0387 - val_acc: 0.9301\n",
      "Epoch 6/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.9742 - acc: 0.9307 - val_loss: 0.9614 - val_acc: 0.9284\n",
      "Epoch 7/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.8618 - acc: 0.9322 - val_loss: 0.7856 - val_acc: 0.9316\n",
      "Epoch 8/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.6041 - acc: 0.9234 - val_loss: 0.4279 - val_acc: 0.9322\n",
      "Epoch 9/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.5879 - acc: 0.9112 - val_loss: 0.6037 - val_acc: 0.9303\n",
      "Epoch 10/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.5499 - acc: 0.9310 - val_loss: 0.4757 - val_acc: 0.9163\n",
      "Epoch 11/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3963 - acc: 0.9153 - val_loss: 0.3222 - val_acc: 0.9297\n",
      "Epoch 12/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3227 - acc: 0.9317 - val_loss: 0.2784 - val_acc: 0.9321\n",
      "Epoch 13/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.8375 - acc: 0.8727 - val_loss: 0.7213 - val_acc: 0.9238\n",
      "Epoch 14/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.7016 - acc: 0.9335 - val_loss: 0.6639 - val_acc: 0.9315\n",
      "Epoch 15/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.5709 - acc: 0.9301 - val_loss: 0.4611 - val_acc: 0.9331\n",
      "Epoch 16/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3921 - acc: 0.9313 - val_loss: 0.3148 - val_acc: 0.9237\n",
      "Epoch 17/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.7895 - acc: 0.8858 - val_loss: 1.0778 - val_acc: 0.9316\n",
      "Epoch 18/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0700 - acc: 0.9326 - val_loss: 1.0731 - val_acc: 0.9317\n",
      "Epoch 19/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.9889 - acc: 0.9324 - val_loss: 0.9740 - val_acc: 0.9316\n",
      "Epoch 20/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.8646 - acc: 0.9305 - val_loss: 0.7413 - val_acc: 0.9308\n",
      "Epoch 21/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.4936 - acc: 0.9313 - val_loss: 0.4611 - val_acc: 0.9326\n",
      "Epoch 22/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4638 - acc: 0.9285 - val_loss: 0.5349 - val_acc: 0.9324\n",
      "Epoch 23/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3718 - acc: 0.9329 - val_loss: 0.2895 - val_acc: 0.9331\n",
      "Epoch 24/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2998 - acc: 0.9350 - val_loss: 0.2700 - val_acc: 0.9328\n",
      "Epoch 25/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3162 - acc: 0.9320 - val_loss: 0.3607 - val_acc: 0.9335\n",
      "Epoch 26/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4136 - acc: 0.9069 - val_loss: 0.4888 - val_acc: 0.9320\n",
      "Epoch 27/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.6943 - acc: 0.9210 - val_loss: 0.6841 - val_acc: 0.9052\n",
      "Epoch 28/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.6096 - acc: 0.9289 - val_loss: 0.5746 - val_acc: 0.9339\n",
      "Epoch 29/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4608 - acc: 0.9335 - val_loss: 0.3738 - val_acc: 0.9343\n",
      "Epoch 30/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3082 - acc: 0.9332 - val_loss: 0.2925 - val_acc: 0.9339\n",
      "Epoch 31/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3954 - acc: 0.9206 - val_loss: 0.4253 - val_acc: 0.9304\n",
      "Epoch 32/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3673 - acc: 0.9247 - val_loss: 0.5464 - val_acc: 0.8193\n",
      "Epoch 33/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.5770 - acc: 0.9183 - val_loss: 0.5531 - val_acc: 0.9256\n",
      "Epoch 34/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.5014 - acc: 0.9278 - val_loss: 0.4340 - val_acc: 0.9008\n",
      "Epoch 35/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4130 - acc: 0.9203 - val_loss: 0.4845 - val_acc: 0.8623\n",
      "Epoch 36/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4490 - acc: 0.9249 - val_loss: 0.4120 - val_acc: 0.8939\n",
      "Epoch 37/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.4389 - acc: 0.9230 - val_loss: 0.3843 - val_acc: 0.9328\n",
      "Epoch 38/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3978 - acc: 0.9181 - val_loss: 0.4749 - val_acc: 0.9316\n",
      "Epoch 39/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3774 - acc: 0.9296 - val_loss: 0.3804 - val_acc: 0.9326\n",
      "Epoch 40/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3299 - acc: 0.9300 - val_loss: 0.2936 - val_acc: 0.9251\n",
      "Epoch 41/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2705 - acc: 0.9303 - val_loss: 0.2350 - val_acc: 0.9330\n",
      "Epoch 42/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2281 - acc: 0.9350 - val_loss: 0.2221 - val_acc: 0.9334\n",
      "Epoch 43/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2213 - acc: 0.9350 - val_loss: 0.2203 - val_acc: 0.9336\n",
      "Epoch 44/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2205 - acc: 0.9347 - val_loss: 0.2272 - val_acc: 0.9313\n",
      "Epoch 45/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3716 - acc: 0.8881 - val_loss: 0.4762 - val_acc: 0.9326\n",
      "Epoch 46/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3530 - acc: 0.9330 - val_loss: 0.2574 - val_acc: 0.9319\n",
      "Epoch 47/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2589 - acc: 0.9348 - val_loss: 0.2451 - val_acc: 0.9341\n",
      "Epoch 48/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2605 - acc: 0.9349 - val_loss: 0.2570 - val_acc: 0.9338\n",
      "Epoch 49/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.2893 - acc: 0.9348 - val_loss: 0.2676 - val_acc: 0.9326\n",
      "Epoch 50/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3772 - acc: 0.9093 - val_loss: 6.6474 - val_acc: 0.2223\n",
      "Epoch 51/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 2.8008 - acc: 0.6988 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 52/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 53/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 54/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 55/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 56/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 57/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 58/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 60/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 61/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 62/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 63/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 64/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 65/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 66/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 67/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 68/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 69/190\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 70/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 71/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 72/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 73/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 74/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 75/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 76/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 77/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 78/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 79/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 80/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 81/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 82/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 83/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 84/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 85/190\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 86/190\n",
      "119999/119999 [==============================] - 6s 51us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 87/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 88/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 89/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 90/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 91/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 92/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 93/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 94/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 95/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 96/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 97/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 98/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 99/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 100/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 101/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 102/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 103/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 104/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 105/190\n",
      "119999/119999 [==============================] - 8s 65us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 106/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 107/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 108/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 109/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 110/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 111/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 112/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 113/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 114/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 115/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 116/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 118/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 119/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 120/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 121/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 122/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 123/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 124/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 125/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 126/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 127/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 128/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 129/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 130/190\n",
      "119999/119999 [==============================] - 8s 68us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 131/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 132/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 133/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 134/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 135/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 136/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 137/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 138/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 139/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 140/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 141/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 142/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 143/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 144/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 145/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 146/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 147/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 148/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 149/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 150/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 151/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 152/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 153/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 154/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 155/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 156/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 157/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 158/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 159/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 160/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 161/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 162/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 163/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 164/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 165/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 166/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 167/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 168/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 169/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 170/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 171/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 172/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 173/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 174/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 175/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 176/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 177/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 178/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 179/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 180/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 181/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 182/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 183/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 184/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 185/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 186/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 187/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 188/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 189/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Epoch 190/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 1.0739 - acc: 0.9334 - val_loss: 1.0912 - val_acc: 0.9323\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/190\n",
      "119999/119999 [==============================] - 13s 111us/step - loss: 0.2707 - acc: 0.9334 - val_loss: 0.2483 - val_acc: 0.9323\n",
      "Epoch 2/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2452 - acc: 0.9334 - val_loss: 0.2460 - val_acc: 0.9323\n",
      "Epoch 3/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2412 - acc: 0.9334 - val_loss: 0.2419 - val_acc: 0.9323\n",
      "Epoch 4/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2366 - acc: 0.9334 - val_loss: 0.2350 - val_acc: 0.9323\n",
      "Epoch 5/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2257 - acc: 0.9334 - val_loss: 0.2192 - val_acc: 0.9327\n",
      "Epoch 6/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2068 - acc: 0.9340 - val_loss: 0.2002 - val_acc: 0.9335\n",
      "Epoch 7/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1931 - acc: 0.9361 - val_loss: 0.1922 - val_acc: 0.9354\n",
      "Epoch 8/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1885 - acc: 0.9362 - val_loss: 0.1892 - val_acc: 0.9356\n",
      "Epoch 9/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1860 - acc: 0.9363 - val_loss: 0.1871 - val_acc: 0.9359\n",
      "Epoch 10/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1843 - acc: 0.9365 - val_loss: 0.1855 - val_acc: 0.9360\n",
      "Epoch 11/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1834 - acc: 0.9368 - val_loss: 0.1854 - val_acc: 0.9364\n",
      "Epoch 12/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1821 - acc: 0.9368 - val_loss: 0.1838 - val_acc: 0.9362\n",
      "Epoch 13/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1814 - acc: 0.9367 - val_loss: 0.1830 - val_acc: 0.9361\n",
      "Epoch 14/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1810 - acc: 0.9367 - val_loss: 0.1825 - val_acc: 0.9364\n",
      "Epoch 15/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1806 - acc: 0.9369 - val_loss: 0.1822 - val_acc: 0.9365\n",
      "Epoch 16/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1806 - acc: 0.9367 - val_loss: 0.1827 - val_acc: 0.9361\n",
      "Epoch 17/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1804 - acc: 0.9366 - val_loss: 0.1851 - val_acc: 0.9360\n",
      "Epoch 18/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1807 - acc: 0.9368 - val_loss: 0.1817 - val_acc: 0.9365\n",
      "Epoch 19/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1802 - acc: 0.9369 - val_loss: 0.1817 - val_acc: 0.9365\n",
      "Epoch 20/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.1817 - val_acc: 0.9364\n",
      "Epoch 21/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1802 - acc: 0.9368 - val_loss: 0.1821 - val_acc: 0.9363\n",
      "Epoch 22/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1800 - acc: 0.9368 - val_loss: 0.1815 - val_acc: 0.9366\n",
      "Epoch 23/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1798 - acc: 0.9367 - val_loss: 0.1816 - val_acc: 0.9364\n",
      "Epoch 24/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1797 - acc: 0.9369 - val_loss: 0.1813 - val_acc: 0.9366\n",
      "Epoch 25/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1797 - acc: 0.9368 - val_loss: 0.1819 - val_acc: 0.9361\n",
      "Epoch 26/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1796 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9365\n",
      "Epoch 27/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1796 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9364\n",
      "Epoch 28/190\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1796 - acc: 0.9369 - val_loss: 0.1817 - val_acc: 0.9361\n",
      "Epoch 29/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1796 - acc: 0.9370 - val_loss: 0.1812 - val_acc: 0.9366\n",
      "Epoch 30/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1798 - acc: 0.9371 - val_loss: 0.1813 - val_acc: 0.9364\n",
      "Epoch 31/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1796 - acc: 0.9370 - val_loss: 0.1815 - val_acc: 0.9365\n",
      "Epoch 32/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9361\n",
      "Epoch 33/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1795 - acc: 0.9369 - val_loss: 0.1818 - val_acc: 0.9362\n",
      "Epoch 34/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1797 - acc: 0.9367 - val_loss: 0.1815 - val_acc: 0.9361\n",
      "Epoch 35/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1794 - acc: 0.9369 - val_loss: 0.1812 - val_acc: 0.9359\n",
      "Epoch 36/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1798 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9364\n",
      "Epoch 37/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1794 - acc: 0.9369 - val_loss: 0.1812 - val_acc: 0.9361\n",
      "Epoch 38/190\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1793 - acc: 0.9370 - val_loss: 0.1825 - val_acc: 0.9358\n",
      "Epoch 39/190\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1795 - acc: 0.9369 - val_loss: 0.1810 - val_acc: 0.9363\n",
      "Epoch 40/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1793 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9364\n",
      "Epoch 41/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1792 - acc: 0.9369 - val_loss: 0.1810 - val_acc: 0.9359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1795 - acc: 0.9369 - val_loss: 0.1810 - val_acc: 0.9361\n",
      "Epoch 43/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1792 - acc: 0.9369 - val_loss: 0.1811 - val_acc: 0.9361\n",
      "Epoch 44/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1793 - acc: 0.9369 - val_loss: 0.1811 - val_acc: 0.9363\n",
      "Epoch 45/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1791 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9362\n",
      "Epoch 46/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1792 - acc: 0.9368 - val_loss: 0.1810 - val_acc: 0.9359\n",
      "Epoch 47/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1791 - acc: 0.9370 - val_loss: 0.1815 - val_acc: 0.9358\n",
      "Epoch 48/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1792 - acc: 0.9368 - val_loss: 0.1811 - val_acc: 0.9359\n",
      "Epoch 49/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1791 - acc: 0.9369 - val_loss: 0.1808 - val_acc: 0.9363\n",
      "Epoch 50/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9359\n",
      "Epoch 51/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1795 - acc: 0.9369 - val_loss: 0.1808 - val_acc: 0.9359\n",
      "Epoch 52/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1808 - val_acc: 0.9359\n",
      "Epoch 53/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1791 - acc: 0.9369 - val_loss: 0.1808 - val_acc: 0.9358\n",
      "Epoch 54/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9360\n",
      "Epoch 55/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1792 - acc: 0.9370 - val_loss: 0.1808 - val_acc: 0.9361\n",
      "Epoch 56/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1791 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9360\n",
      "Epoch 57/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1790 - acc: 0.9371 - val_loss: 0.1813 - val_acc: 0.9360\n",
      "Epoch 58/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1790 - acc: 0.9369 - val_loss: 0.1807 - val_acc: 0.9358\n",
      "Epoch 59/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1789 - acc: 0.9370 - val_loss: 0.1807 - val_acc: 0.9360\n",
      "Epoch 60/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1788 - acc: 0.9369 - val_loss: 0.1807 - val_acc: 0.9359\n",
      "Epoch 61/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1788 - acc: 0.9369 - val_loss: 0.1810 - val_acc: 0.9360\n",
      "Epoch 62/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1792 - acc: 0.9370 - val_loss: 0.1812 - val_acc: 0.9361\n",
      "Epoch 63/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1788 - acc: 0.9370 - val_loss: 0.1808 - val_acc: 0.9360\n",
      "Epoch 64/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1788 - acc: 0.9369 - val_loss: 0.1809 - val_acc: 0.9360\n",
      "Epoch 65/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1789 - acc: 0.9369 - val_loss: 0.1806 - val_acc: 0.9360\n",
      "Epoch 66/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1789 - acc: 0.9369 - val_loss: 0.1808 - val_acc: 0.9358\n",
      "Epoch 67/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1810 - val_acc: 0.9360\n",
      "Epoch 68/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1791 - acc: 0.9371 - val_loss: 0.1817 - val_acc: 0.9360\n",
      "Epoch 69/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1791 - acc: 0.9371 - val_loss: 0.1806 - val_acc: 0.9362\n",
      "Epoch 70/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1790 - acc: 0.9369 - val_loss: 0.1809 - val_acc: 0.9360\n",
      "Epoch 71/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1786 - acc: 0.9369 - val_loss: 0.1806 - val_acc: 0.9361\n",
      "Epoch 72/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1786 - acc: 0.9370 - val_loss: 0.1805 - val_acc: 0.9362\n",
      "Epoch 73/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1805 - val_acc: 0.9363\n",
      "Epoch 74/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1784 - acc: 0.9371 - val_loss: 0.1805 - val_acc: 0.9361\n",
      "Epoch 75/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1806 - val_acc: 0.9361\n",
      "Epoch 76/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1787 - acc: 0.9369 - val_loss: 0.1804 - val_acc: 0.9361\n",
      "Epoch 77/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1786 - acc: 0.9372 - val_loss: 0.1806 - val_acc: 0.9361\n",
      "Epoch 78/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.1804 - val_acc: 0.9362\n",
      "Epoch 79/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1785 - acc: 0.9372 - val_loss: 0.1805 - val_acc: 0.9363\n",
      "Epoch 80/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.1805 - val_acc: 0.9362\n",
      "Epoch 81/190\n",
      "119999/119999 [==============================] - 9s 71us/step - loss: 0.1787 - acc: 0.9370 - val_loss: 0.1804 - val_acc: 0.9361\n",
      "Epoch 82/190\n",
      "119999/119999 [==============================] - 8s 67us/step - loss: 0.1784 - acc: 0.9371 - val_loss: 0.1803 - val_acc: 0.9362\n",
      "Epoch 83/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9362\n",
      "Epoch 84/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1782 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9364\n",
      "Epoch 85/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9363\n",
      "Epoch 86/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1785 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9361\n",
      "Epoch 87/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1808 - val_acc: 0.9363\n",
      "Epoch 88/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1783 - acc: 0.9370 - val_loss: 0.1802 - val_acc: 0.9365\n",
      "Epoch 89/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1788 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9364\n",
      "Epoch 90/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9363\n",
      "Epoch 91/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1783 - acc: 0.9373 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 92/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1782 - acc: 0.9371 - val_loss: 0.1801 - val_acc: 0.9366\n",
      "Epoch 93/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1781 - acc: 0.9372 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 94/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1782 - acc: 0.9372 - val_loss: 0.1804 - val_acc: 0.9363\n",
      "Epoch 95/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 96/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1782 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9365\n",
      "Epoch 97/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1786 - acc: 0.9374 - val_loss: 0.1802 - val_acc: 0.9364\n",
      "Epoch 98/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9367\n",
      "Epoch 99/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1783 - acc: 0.9371 - val_loss: 0.1805 - val_acc: 0.9363\n",
      "Epoch 100/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1784 - acc: 0.9373 - val_loss: 0.1800 - val_acc: 0.9365\n",
      "Epoch 101/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1785 - acc: 0.9371 - val_loss: 0.1801 - val_acc: 0.9366\n",
      "Epoch 102/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1781 - acc: 0.9371 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 103/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1780 - acc: 0.9373 - val_loss: 0.1818 - val_acc: 0.9361\n",
      "Epoch 104/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1783 - acc: 0.9373 - val_loss: 0.1799 - val_acc: 0.9367\n",
      "Epoch 105/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1781 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 106/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1779 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9367\n",
      "Epoch 107/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1782 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9363\n",
      "Epoch 108/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1784 - acc: 0.9371 - val_loss: 0.1799 - val_acc: 0.9365\n",
      "Epoch 109/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1779 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9364\n",
      "Epoch 110/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9365\n",
      "Epoch 111/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1779 - acc: 0.9375 - val_loss: 0.1799 - val_acc: 0.9365\n",
      "Epoch 112/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1779 - acc: 0.9372 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 113/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1780 - acc: 0.9374 - val_loss: 0.1805 - val_acc: 0.9361\n",
      "Epoch 114/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1784 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 115/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9365\n",
      "Epoch 116/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9365\n",
      "Epoch 117/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1787 - acc: 0.9375 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 118/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1783 - acc: 0.9375 - val_loss: 0.1804 - val_acc: 0.9365\n",
      "Epoch 119/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1785 - acc: 0.9373 - val_loss: 0.1808 - val_acc: 0.9359\n",
      "Epoch 120/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9368\n",
      "Epoch 121/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1778 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9366\n",
      "Epoch 122/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1781 - acc: 0.9374 - val_loss: 0.1809 - val_acc: 0.9358\n",
      "Epoch 123/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1778 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 124/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1777 - acc: 0.9374 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 125/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1778 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9367\n",
      "Epoch 126/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1778 - acc: 0.9375 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 127/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1779 - acc: 0.9372 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 128/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1777 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9365\n",
      "Epoch 129/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1780 - acc: 0.9376 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 130/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1783 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9364\n",
      "Epoch 131/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1778 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 132/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1778 - acc: 0.9373 - val_loss: 0.1799 - val_acc: 0.9364\n",
      "Epoch 133/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9364\n",
      "Epoch 134/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1778 - acc: 0.9375 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 135/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9367\n",
      "Epoch 136/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1776 - acc: 0.9373 - val_loss: 0.1797 - val_acc: 0.9367\n",
      "Epoch 137/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1779 - acc: 0.9370 - val_loss: 0.1807 - val_acc: 0.9365\n",
      "Epoch 138/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.1798 - val_acc: 0.9366\n",
      "Epoch 139/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1775 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 140/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1776 - acc: 0.9371 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 141/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1777 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9366\n",
      "Epoch 142/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9369\n",
      "Epoch 143/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1778 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9368\n",
      "Epoch 144/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9366\n",
      "Epoch 145/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1801 - val_acc: 0.9366\n",
      "Epoch 146/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1782 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9365\n",
      "Epoch 147/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9364\n",
      "Epoch 148/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9369\n",
      "Epoch 149/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9378 - val_loss: 0.1801 - val_acc: 0.9367\n",
      "Epoch 150/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1779 - acc: 0.9375 - val_loss: 0.1799 - val_acc: 0.9363\n",
      "Epoch 151/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1775 - acc: 0.9376 - val_loss: 0.1797 - val_acc: 0.9361\n",
      "Epoch 152/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 153/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1801 - val_acc: 0.9366\n",
      "Epoch 154/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1782 - acc: 0.9371 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 155/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1780 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9366\n",
      "Epoch 156/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1776 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 157/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1801 - val_acc: 0.9362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9366\n",
      "Epoch 159/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1775 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9365\n",
      "Epoch 160/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1774 - acc: 0.9376 - val_loss: 0.1804 - val_acc: 0.9365\n",
      "Epoch 161/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1776 - acc: 0.9377 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 162/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1775 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9363\n",
      "Epoch 163/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1776 - acc: 0.9377 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 164/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1773 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 165/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 166/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1774 - acc: 0.9376 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 167/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1774 - acc: 0.9377 - val_loss: 0.1801 - val_acc: 0.9363\n",
      "Epoch 168/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 169/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1775 - acc: 0.9376 - val_loss: 0.1807 - val_acc: 0.9366\n",
      "Epoch 170/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 171/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 172/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1802 - val_acc: 0.9367\n",
      "Epoch 173/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1779 - acc: 0.9376 - val_loss: 0.1817 - val_acc: 0.9362\n",
      "Epoch 174/190\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 175/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9365\n",
      "Epoch 176/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1773 - acc: 0.9374 - val_loss: 0.1795 - val_acc: 0.9364\n",
      "Epoch 177/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1774 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 178/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 179/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1773 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 180/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1773 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9362\n",
      "Epoch 181/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1773 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9364\n",
      "Epoch 182/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1774 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9363\n",
      "Epoch 183/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1807 - val_acc: 0.9363\n",
      "Epoch 184/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 185/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1777 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9366\n",
      "Epoch 186/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1773 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 187/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1772 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9365\n",
      "Epoch 188/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1772 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9363\n",
      "Epoch 189/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1773 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9365\n",
      "Epoch 190/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1772 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/190\n",
      "119999/119999 [==============================] - 12s 99us/step - loss: 1.0510 - acc: 0.2065 - val_loss: 0.6379 - val_acc: 0.4067\n",
      "Epoch 2/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.6023 - acc: 0.4403 - val_loss: 0.8038 - val_acc: 0.0847\n",
      "Epoch 3/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.6946 - acc: 0.2319 - val_loss: 0.5225 - val_acc: 0.5530\n",
      "Epoch 4/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.8788 - acc: 0.4048 - val_loss: 0.9921 - val_acc: 0.0533\n",
      "Epoch 5/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.7533 - acc: 0.3728 - val_loss: 0.3981 - val_acc: 0.8626\n",
      "Epoch 6/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 2.1215 - acc: 0.4020 - val_loss: 2.9249 - val_acc: 0.0678\n",
      "Epoch 7/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 1.4296 - acc: 0.2834 - val_loss: 0.6300 - val_acc: 0.9267\n",
      "Epoch 8/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2945 - acc: 0.9301 - val_loss: 0.2437 - val_acc: 0.9285\n",
      "Epoch 9/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2392 - acc: 0.9305 - val_loss: 0.2399 - val_acc: 0.9284\n",
      "Epoch 10/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2328 - acc: 0.9307 - val_loss: 0.2311 - val_acc: 0.9293\n",
      "Epoch 11/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.2226 - acc: 0.9313 - val_loss: 0.2222 - val_acc: 0.9302\n",
      "Epoch 12/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2151 - acc: 0.9320 - val_loss: 0.2175 - val_acc: 0.9314\n",
      "Epoch 13/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2100 - acc: 0.9327 - val_loss: 0.2108 - val_acc: 0.9321\n",
      "Epoch 14/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2072 - acc: 0.9329 - val_loss: 0.2063 - val_acc: 0.9325\n",
      "Epoch 15/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2025 - acc: 0.9332 - val_loss: 0.2350 - val_acc: 0.9323\n",
      "Epoch 16/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2229 - acc: 0.9330 - val_loss: 0.2214 - val_acc: 0.9326\n",
      "Epoch 17/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2023 - acc: 0.9332 - val_loss: 0.1993 - val_acc: 0.9326\n",
      "Epoch 18/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1934 - acc: 0.9333 - val_loss: 0.2015 - val_acc: 0.9327\n",
      "Epoch 19/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1939 - acc: 0.9329 - val_loss: 0.1975 - val_acc: 0.9324\n",
      "Epoch 20/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2172 - acc: 0.9329 - val_loss: 0.2040 - val_acc: 0.9324\n",
      "Epoch 21/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2004 - acc: 0.9334 - val_loss: 0.1969 - val_acc: 0.9327\n",
      "Epoch 22/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1919 - acc: 0.9334 - val_loss: 0.1954 - val_acc: 0.9327\n",
      "Epoch 23/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1898 - acc: 0.9334 - val_loss: 0.1962 - val_acc: 0.9326\n",
      "Epoch 24/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1908 - acc: 0.9334 - val_loss: 0.1935 - val_acc: 0.9326\n",
      "Epoch 25/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1894 - acc: 0.9334 - val_loss: 0.1911 - val_acc: 0.9327\n",
      "Epoch 26/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1903 - acc: 0.9333 - val_loss: 0.1908 - val_acc: 0.9327\n",
      "Epoch 27/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1877 - acc: 0.9334 - val_loss: 0.1902 - val_acc: 0.9327\n",
      "Epoch 28/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1880 - acc: 0.9335 - val_loss: 0.1904 - val_acc: 0.9327\n",
      "Epoch 29/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1891 - acc: 0.9335 - val_loss: 0.1947 - val_acc: 0.9327\n",
      "Epoch 30/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1879 - acc: 0.9334 - val_loss: 0.1908 - val_acc: 0.9327\n",
      "Epoch 31/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2044 - acc: 0.9333 - val_loss: 0.2102 - val_acc: 0.9326\n",
      "Epoch 32/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1959 - acc: 0.9333 - val_loss: 0.1926 - val_acc: 0.9326\n",
      "Epoch 33/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2141 - acc: 0.9333 - val_loss: 0.2360 - val_acc: 0.9324\n",
      "Epoch 34/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2381 - acc: 0.9247 - val_loss: 0.2416 - val_acc: 0.9148\n",
      "Epoch 35/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.3014 - acc: 0.9139 - val_loss: 0.2450 - val_acc: 0.9154\n",
      "Epoch 36/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.3970 - acc: 0.9135 - val_loss: 0.2213 - val_acc: 0.9240\n",
      "Epoch 37/190\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.8469 - acc: 0.3675 - val_loss: 1.0416 - val_acc: 0.0757\n",
      "Epoch 38/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.5790 - acc: 0.6270 - val_loss: 0.2617 - val_acc: 0.9189\n",
      "Epoch 39/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2391 - acc: 0.9198 - val_loss: 0.2504 - val_acc: 0.9156\n",
      "Epoch 40/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.2158 - acc: 0.9172 - val_loss: 0.2129 - val_acc: 0.9157\n",
      "Epoch 41/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2044 - acc: 0.9195 - val_loss: 0.1984 - val_acc: 0.9210\n",
      "Epoch 42/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2221 - acc: 0.9252 - val_loss: 0.2125 - val_acc: 0.9271\n",
      "Epoch 43/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2104 - acc: 0.9284 - val_loss: 0.1998 - val_acc: 0.9273\n",
      "Epoch 44/190\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1947 - acc: 0.9283 - val_loss: 0.1978 - val_acc: 0.9273\n",
      "Epoch 45/190\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1910 - acc: 0.9281 - val_loss: 0.1907 - val_acc: 0.9270\n",
      "Epoch 46/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1891 - acc: 0.9281 - val_loss: 0.1897 - val_acc: 0.9272\n",
      "Epoch 47/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1871 - acc: 0.9283 - val_loss: 0.1893 - val_acc: 0.9274\n",
      "Epoch 48/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1869 - acc: 0.9285 - val_loss: 0.1890 - val_acc: 0.9276\n",
      "Epoch 49/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1872 - acc: 0.9286 - val_loss: 0.1901 - val_acc: 0.9276\n",
      "Epoch 50/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1890 - acc: 0.9285 - val_loss: 0.1956 - val_acc: 0.9277\n",
      "Epoch 51/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1892 - acc: 0.9285 - val_loss: 0.1920 - val_acc: 0.9273\n",
      "Epoch 52/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1871 - acc: 0.9286 - val_loss: 0.1894 - val_acc: 0.9276\n",
      "Epoch 53/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1861 - acc: 0.9287 - val_loss: 0.1898 - val_acc: 0.9276\n",
      "Epoch 54/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1857 - acc: 0.9287 - val_loss: 0.1926 - val_acc: 0.9278\n",
      "Epoch 55/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1884 - acc: 0.9289 - val_loss: 0.1886 - val_acc: 0.9277\n",
      "Epoch 56/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1861 - acc: 0.9288 - val_loss: 0.1871 - val_acc: 0.9277\n",
      "Epoch 57/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1851 - acc: 0.9288 - val_loss: 0.1871 - val_acc: 0.9277\n",
      "Epoch 58/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1848 - acc: 0.9289 - val_loss: 0.1867 - val_acc: 0.9277\n",
      "Epoch 59/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1847 - acc: 0.9289 - val_loss: 0.1871 - val_acc: 0.9278\n",
      "Epoch 60/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.2049 - acc: 0.9293 - val_loss: 0.2205 - val_acc: 0.9283\n",
      "Epoch 61/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1974 - acc: 0.9293 - val_loss: 0.1958 - val_acc: 0.9284\n",
      "Epoch 62/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1905 - acc: 0.9292 - val_loss: 0.1898 - val_acc: 0.9284\n",
      "Epoch 63/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1852 - acc: 0.9290 - val_loss: 0.1862 - val_acc: 0.9292\n",
      "Epoch 64/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1834 - acc: 0.9304 - val_loss: 0.1864 - val_acc: 0.9302\n",
      "Epoch 65/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1830 - acc: 0.9306 - val_loss: 0.1860 - val_acc: 0.9301\n",
      "Epoch 66/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1831 - acc: 0.9304 - val_loss: 0.1873 - val_acc: 0.9300\n",
      "Epoch 67/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2013 - acc: 0.9307 - val_loss: 0.1988 - val_acc: 0.9302\n",
      "Epoch 68/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1874 - acc: 0.9305 - val_loss: 0.1875 - val_acc: 0.9299\n",
      "Epoch 69/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1840 - acc: 0.9303 - val_loss: 0.1855 - val_acc: 0.9297\n",
      "Epoch 70/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1831 - acc: 0.9303 - val_loss: 0.1853 - val_acc: 0.9300\n",
      "Epoch 71/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1832 - acc: 0.9305 - val_loss: 0.1858 - val_acc: 0.9301\n",
      "Epoch 72/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1835 - acc: 0.9306 - val_loss: 0.1881 - val_acc: 0.9301\n",
      "Epoch 73/190\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1826 - acc: 0.9305 - val_loss: 0.1853 - val_acc: 0.9301\n",
      "Epoch 74/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1828 - acc: 0.9305 - val_loss: 0.1852 - val_acc: 0.9301\n",
      "Epoch 75/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1828 - acc: 0.9305 - val_loss: 0.1849 - val_acc: 0.9301\n",
      "Epoch 76/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1822 - acc: 0.9305 - val_loss: 0.1846 - val_acc: 0.9302\n",
      "Epoch 77/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1825 - acc: 0.9307 - val_loss: 0.1873 - val_acc: 0.9303\n",
      "Epoch 78/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1830 - acc: 0.9308 - val_loss: 0.1844 - val_acc: 0.9303\n",
      "Epoch 79/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1823 - acc: 0.9307 - val_loss: 0.1848 - val_acc: 0.9302\n",
      "Epoch 80/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1817 - acc: 0.9306 - val_loss: 0.1842 - val_acc: 0.9302\n",
      "Epoch 81/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1837 - acc: 0.9307 - val_loss: 0.2621 - val_acc: 0.9331\n",
      "Epoch 82/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2583 - acc: 0.9321 - val_loss: 0.2265 - val_acc: 0.9306\n",
      "Epoch 83/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2079 - acc: 0.9308 - val_loss: 0.1947 - val_acc: 0.9302\n",
      "Epoch 84/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2603 - acc: 0.9295 - val_loss: 0.2422 - val_acc: 0.9290\n",
      "Epoch 85/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2239 - acc: 0.9296 - val_loss: 0.2092 - val_acc: 0.9285\n",
      "Epoch 86/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1963 - acc: 0.9290 - val_loss: 0.1908 - val_acc: 0.9281\n",
      "Epoch 87/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1910 - acc: 0.9287 - val_loss: 0.1984 - val_acc: 0.9281\n",
      "Epoch 88/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1877 - acc: 0.9290 - val_loss: 0.1869 - val_acc: 0.9285\n",
      "Epoch 89/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1885 - acc: 0.9294 - val_loss: 0.2800 - val_acc: 0.9316\n",
      "Epoch 90/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2480 - acc: 0.9321 - val_loss: 0.2386 - val_acc: 0.9321\n",
      "Epoch 91/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.2061 - acc: 0.9315 - val_loss: 0.1883 - val_acc: 0.9305\n",
      "Epoch 92/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1862 - acc: 0.9308 - val_loss: 0.1869 - val_acc: 0.9302\n",
      "Epoch 93/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1829 - acc: 0.9306 - val_loss: 0.1858 - val_acc: 0.9302\n",
      "Epoch 94/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1822 - acc: 0.9306 - val_loss: 0.1855 - val_acc: 0.9303\n",
      "Epoch 95/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1820 - acc: 0.9307 - val_loss: 0.1854 - val_acc: 0.9303\n",
      "Epoch 96/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1982 - acc: 0.9319 - val_loss: 0.2088 - val_acc: 0.9309\n",
      "Epoch 97/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2014 - acc: 0.9315 - val_loss: 0.2473 - val_acc: 0.9333\n",
      "Epoch 98/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.3205 - acc: 0.9318 - val_loss: 0.6437 - val_acc: 0.9184\n",
      "Epoch 99/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.4363 - acc: 0.9291 - val_loss: 0.2262 - val_acc: 0.9304\n",
      "Epoch 100/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2176 - acc: 0.9320 - val_loss: 0.1904 - val_acc: 0.9303\n",
      "Epoch 101/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1849 - acc: 0.9306 - val_loss: 0.1853 - val_acc: 0.9303\n",
      "Epoch 102/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1822 - acc: 0.9307 - val_loss: 0.1854 - val_acc: 0.9304\n",
      "Epoch 103/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1817 - acc: 0.9307 - val_loss: 0.1849 - val_acc: 0.9303\n",
      "Epoch 104/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1817 - acc: 0.9307 - val_loss: 0.1843 - val_acc: 0.9304\n",
      "Epoch 105/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1819 - acc: 0.9307 - val_loss: 0.1842 - val_acc: 0.9303\n",
      "Epoch 106/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1835 - acc: 0.9311 - val_loss: 0.1875 - val_acc: 0.9305\n",
      "Epoch 107/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1835 - acc: 0.9318 - val_loss: 0.1851 - val_acc: 0.9309\n",
      "Epoch 108/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1821 - acc: 0.9327 - val_loss: 0.1850 - val_acc: 0.9317\n",
      "Epoch 109/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2292 - acc: 0.9310 - val_loss: 0.4658 - val_acc: 0.9258\n",
      "Epoch 110/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.3733 - acc: 0.9327 - val_loss: 0.2621 - val_acc: 0.9328\n",
      "Epoch 111/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.2264 - acc: 0.9343 - val_loss: 0.2058 - val_acc: 0.9342\n",
      "Epoch 112/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1984 - acc: 0.9332 - val_loss: 0.1974 - val_acc: 0.9314\n",
      "Epoch 113/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1869 - acc: 0.9320 - val_loss: 0.1861 - val_acc: 0.9309\n",
      "Epoch 114/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1849 - acc: 0.9318 - val_loss: 0.1850 - val_acc: 0.9313\n",
      "Epoch 115/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1859 - acc: 0.9318 - val_loss: 0.1865 - val_acc: 0.9311\n",
      "Epoch 116/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1907 - acc: 0.9316 - val_loss: 0.3641 - val_acc: 0.9267\n",
      "Epoch 117/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.3947 - acc: 0.9321 - val_loss: 0.2190 - val_acc: 0.9320\n",
      "Epoch 118/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2777 - acc: 0.9336 - val_loss: 0.6278 - val_acc: 0.8726\n",
      "Epoch 119/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.5470 - acc: 0.6599 - val_loss: 0.6952 - val_acc: 0.9307\n",
      "Epoch 120/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.4521 - acc: 0.9320 - val_loss: 0.2312 - val_acc: 0.9314\n",
      "Epoch 121/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2079 - acc: 0.9318 - val_loss: 0.1863 - val_acc: 0.9309\n",
      "Epoch 122/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1896 - acc: 0.9318 - val_loss: 0.1876 - val_acc: 0.9307\n",
      "Epoch 123/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1851 - acc: 0.9317 - val_loss: 0.1867 - val_acc: 0.9311\n",
      "Epoch 124/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1837 - acc: 0.9318 - val_loss: 0.1849 - val_acc: 0.9312\n",
      "Epoch 125/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1824 - acc: 0.9317 - val_loss: 0.1844 - val_acc: 0.9312\n",
      "Epoch 126/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1813 - acc: 0.9319 - val_loss: 0.1840 - val_acc: 0.9314\n",
      "Epoch 127/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1811 - acc: 0.9321 - val_loss: 0.1842 - val_acc: 0.9315\n",
      "Epoch 128/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1820 - acc: 0.9321 - val_loss: 0.1834 - val_acc: 0.9315\n",
      "Epoch 129/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1814 - acc: 0.9324 - val_loss: 0.1831 - val_acc: 0.9315\n",
      "Epoch 130/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.2025 - acc: 0.9335 - val_loss: 0.1951 - val_acc: 0.9318\n",
      "Epoch 131/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1922 - acc: 0.9333 - val_loss: 0.1934 - val_acc: 0.9319\n",
      "Epoch 132/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1898 - acc: 0.9340 - val_loss: 0.1951 - val_acc: 0.9315\n",
      "Epoch 133/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1826 - acc: 0.9343 - val_loss: 0.1832 - val_acc: 0.9328\n",
      "Epoch 134/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1802 - acc: 0.9346 - val_loss: 0.1832 - val_acc: 0.9336\n",
      "Epoch 135/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1816 - acc: 0.9349 - val_loss: 0.1898 - val_acc: 0.9339\n",
      "Epoch 136/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1889 - acc: 0.9348 - val_loss: 0.2024 - val_acc: 0.9339\n",
      "Epoch 137/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1892 - acc: 0.9347 - val_loss: 0.1846 - val_acc: 0.9337\n",
      "Epoch 138/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1839 - acc: 0.9347 - val_loss: 0.1881 - val_acc: 0.9336\n",
      "Epoch 139/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1842 - acc: 0.9350 - val_loss: 0.1914 - val_acc: 0.9331\n",
      "Epoch 140/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1845 - acc: 0.9349 - val_loss: 0.1837 - val_acc: 0.9338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1819 - acc: 0.9352 - val_loss: 0.1845 - val_acc: 0.9346\n",
      "Epoch 142/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1799 - acc: 0.9350 - val_loss: 0.1818 - val_acc: 0.9346\n",
      "Epoch 143/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1796 - acc: 0.9352 - val_loss: 0.1820 - val_acc: 0.9344\n",
      "Epoch 144/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1816 - acc: 0.9352 - val_loss: 0.1823 - val_acc: 0.9346\n",
      "Epoch 145/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1803 - acc: 0.9350 - val_loss: 0.1820 - val_acc: 0.9344\n",
      "Epoch 146/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1794 - acc: 0.9352 - val_loss: 0.1823 - val_acc: 0.9343\n",
      "Epoch 147/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1859 - acc: 0.9352 - val_loss: 0.1998 - val_acc: 0.9341\n",
      "Epoch 148/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1861 - acc: 0.9358 - val_loss: 0.1849 - val_acc: 0.9341\n",
      "Epoch 149/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1805 - acc: 0.9359 - val_loss: 0.1820 - val_acc: 0.9340\n",
      "Epoch 150/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1795 - acc: 0.9359 - val_loss: 0.1819 - val_acc: 0.9352\n",
      "Epoch 151/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1790 - acc: 0.9359 - val_loss: 0.1819 - val_acc: 0.9349\n",
      "Epoch 152/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2044 - acc: 0.9357 - val_loss: 0.2131 - val_acc: 0.9327\n",
      "Epoch 153/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2145 - acc: 0.9337 - val_loss: 0.3104 - val_acc: 0.9290\n",
      "Epoch 154/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2398 - acc: 0.9343 - val_loss: 0.2059 - val_acc: 0.9338\n",
      "Epoch 155/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1909 - acc: 0.9346 - val_loss: 0.1880 - val_acc: 0.9334\n",
      "Epoch 156/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1904 - acc: 0.9352 - val_loss: 0.1906 - val_acc: 0.9336\n",
      "Epoch 157/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1852 - acc: 0.9351 - val_loss: 0.1846 - val_acc: 0.9344\n",
      "Epoch 158/190\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1816 - acc: 0.9352 - val_loss: 0.1866 - val_acc: 0.9343\n",
      "Epoch 159/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1845 - acc: 0.9358 - val_loss: 0.1836 - val_acc: 0.9345\n",
      "Epoch 160/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1812 - acc: 0.9357 - val_loss: 0.1849 - val_acc: 0.9345\n",
      "Epoch 161/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1810 - acc: 0.9357 - val_loss: 0.1842 - val_acc: 0.9344\n",
      "Epoch 162/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1830 - acc: 0.9359 - val_loss: 0.1827 - val_acc: 0.9345\n",
      "Epoch 163/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1801 - acc: 0.9358 - val_loss: 0.1825 - val_acc: 0.9344\n",
      "Epoch 164/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1795 - acc: 0.9359 - val_loss: 0.1817 - val_acc: 0.9345\n",
      "Epoch 165/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1797 - acc: 0.9360 - val_loss: 0.1817 - val_acc: 0.9346\n",
      "Epoch 166/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1810 - acc: 0.9359 - val_loss: 0.1823 - val_acc: 0.9348\n",
      "Epoch 167/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.2474 - acc: 0.9349 - val_loss: 0.2618 - val_acc: 0.9351\n",
      "Epoch 168/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.6659 - acc: 0.6735 - val_loss: 2.3508 - val_acc: 0.0679\n",
      "Epoch 169/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 2.9527 - acc: 0.0672 - val_loss: 2.7884 - val_acc: 0.0689\n",
      "Epoch 170/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 1.7756 - acc: 0.3096 - val_loss: 0.9805 - val_acc: 0.1643\n",
      "Epoch 171/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.5911 - acc: 0.4194 - val_loss: 0.4068 - val_acc: 0.8155\n",
      "Epoch 172/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2852 - acc: 0.8786 - val_loss: 0.2396 - val_acc: 0.8943\n",
      "Epoch 173/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2367 - acc: 0.9060 - val_loss: 0.2083 - val_acc: 0.9099\n",
      "Epoch 174/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2033 - acc: 0.9117 - val_loss: 0.1978 - val_acc: 0.9121\n",
      "Epoch 175/190\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1895 - acc: 0.9165 - val_loss: 0.1933 - val_acc: 0.9156\n",
      "Epoch 176/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1851 - acc: 0.9190 - val_loss: 0.1876 - val_acc: 0.9193\n",
      "Epoch 177/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1841 - acc: 0.9205 - val_loss: 0.1866 - val_acc: 0.9188\n",
      "Epoch 178/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1841 - acc: 0.9205 - val_loss: 0.1885 - val_acc: 0.9195\n",
      "Epoch 179/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1845 - acc: 0.9210 - val_loss: 0.1878 - val_acc: 0.9194\n",
      "Epoch 180/190\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1828 - acc: 0.9215 - val_loss: 0.1853 - val_acc: 0.9198\n",
      "Epoch 181/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1820 - acc: 0.9217 - val_loss: 0.1858 - val_acc: 0.9205\n",
      "Epoch 182/190\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1906 - acc: 0.9238 - val_loss: 0.2184 - val_acc: 0.9280\n",
      "Epoch 183/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2102 - acc: 0.9290 - val_loss: 0.2010 - val_acc: 0.9279\n",
      "Epoch 184/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1962 - acc: 0.9302 - val_loss: 0.2071 - val_acc: 0.9291\n",
      "Epoch 185/190\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1895 - acc: 0.9290 - val_loss: 0.1877 - val_acc: 0.9285\n",
      "Epoch 186/190\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1846 - acc: 0.9292 - val_loss: 0.1861 - val_acc: 0.9285\n",
      "Epoch 187/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1829 - acc: 0.9294 - val_loss: 0.1841 - val_acc: 0.9282\n",
      "Epoch 188/190\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1808 - acc: 0.9293 - val_loss: 0.1840 - val_acc: 0.9286\n",
      "Epoch 189/190\n",
      "119999/119999 [==============================] - 6s 52us/step - loss: 0.1804 - acc: 0.9297 - val_loss: 0.1831 - val_acc: 0.9293\n",
      "Epoch 190/190\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1803 - acc: 0.9302 - val_loss: 0.1845 - val_acc: 0.9307\n"
     ]
    }
   ],
   "source": [
    "activation_function = ['relu', 'sigmoid', 'tanh']\n",
    "scores = []\n",
    "for i in range(len(activation_function)):\n",
    "    act_model = Sequential()\n",
    "    act_model.add(Dense(units = 600, input_dim = 10, activation = activation_function[i]))\n",
    "    act_model.add(Dense(units = 300, activation = activation_function[i]))\n",
    "    act_model.add(Dense(units = 100, activation = activation_function[i]))\n",
    "    act_model.add(Dense(units = 1, activation = activation_function[i]))\n",
    "\n",
    "    act_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    act_log = act_model.fit(X_train, Y_train, \n",
    "            batch_size = 6000, epochs= 190, verbose = 1,\n",
    "                            validation_data = (X_validation, Y_validation))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Activation Function = 'Sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix Parameter DNN after Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119999 samples, validate on 30000 samples\n",
      "Epoch 1/200\n",
      "119999/119999 [==============================] - 13s 110us/step - loss: 0.2716 - acc: 0.9334 - val_loss: 0.2470 - val_acc: 0.9323\n",
      "Epoch 2/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.2455 - acc: 0.9334 - val_loss: 0.2469 - val_acc: 0.9323\n",
      "Epoch 3/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.2429 - acc: 0.9334 - val_loss: 0.2444 - val_acc: 0.9323\n",
      "Epoch 4/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2402 - acc: 0.9334 - val_loss: 0.2406 - val_acc: 0.9323\n",
      "Epoch 5/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2343 - acc: 0.9334 - val_loss: 0.2311 - val_acc: 0.9323\n",
      "Epoch 6/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.2198 - acc: 0.9334 - val_loss: 0.2112 - val_acc: 0.9327\n",
      "Epoch 7/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1997 - acc: 0.9348 - val_loss: 0.1950 - val_acc: 0.9347\n",
      "Epoch 8/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1902 - acc: 0.9363 - val_loss: 0.1907 - val_acc: 0.9353\n",
      "Epoch 9/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1870 - acc: 0.9363 - val_loss: 0.1877 - val_acc: 0.9357\n",
      "Epoch 10/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1850 - acc: 0.9365 - val_loss: 0.1863 - val_acc: 0.9357\n",
      "Epoch 11/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1837 - acc: 0.9365 - val_loss: 0.1850 - val_acc: 0.9357\n",
      "Epoch 12/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1824 - acc: 0.9367 - val_loss: 0.1838 - val_acc: 0.9360\n",
      "Epoch 13/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1817 - acc: 0.9367 - val_loss: 0.1831 - val_acc: 0.9362\n",
      "Epoch 14/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1810 - acc: 0.9366 - val_loss: 0.1826 - val_acc: 0.9361\n",
      "Epoch 15/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1808 - acc: 0.9367 - val_loss: 0.1823 - val_acc: 0.9362\n",
      "Epoch 16/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1805 - acc: 0.9368 - val_loss: 0.1821 - val_acc: 0.9365\n",
      "Epoch 17/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1803 - acc: 0.9368 - val_loss: 0.1823 - val_acc: 0.9360\n",
      "Epoch 18/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1803 - acc: 0.9369 - val_loss: 0.1828 - val_acc: 0.9361\n",
      "Epoch 19/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1801 - acc: 0.9368 - val_loss: 0.1816 - val_acc: 0.9364\n",
      "Epoch 20/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.1821 - val_acc: 0.9361\n",
      "Epoch 21/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1802 - acc: 0.9368 - val_loss: 0.1814 - val_acc: 0.9364\n",
      "Epoch 22/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1798 - acc: 0.9368 - val_loss: 0.1815 - val_acc: 0.9361\n",
      "Epoch 23/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1797 - acc: 0.9368 - val_loss: 0.1814 - val_acc: 0.9363\n",
      "Epoch 24/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9365\n",
      "Epoch 25/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1797 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9361\n",
      "Epoch 26/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1796 - acc: 0.9369 - val_loss: 0.1813 - val_acc: 0.9361\n",
      "Epoch 27/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1797 - acc: 0.9368 - val_loss: 0.1814 - val_acc: 0.9366\n",
      "Epoch 28/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1797 - acc: 0.9369 - val_loss: 0.1813 - val_acc: 0.9363\n",
      "Epoch 29/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1797 - acc: 0.9370 - val_loss: 0.1812 - val_acc: 0.9362\n",
      "Epoch 30/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1796 - acc: 0.9369 - val_loss: 0.1816 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1800 - acc: 0.9368 - val_loss: 0.1819 - val_acc: 0.9359\n",
      "Epoch 32/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1804 - acc: 0.9367 - val_loss: 0.1811 - val_acc: 0.9359\n",
      "Epoch 33/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1794 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9363\n",
      "Epoch 34/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1797 - acc: 0.9368 - val_loss: 0.1813 - val_acc: 0.9359\n",
      "Epoch 35/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1796 - acc: 0.9369 - val_loss: 0.1820 - val_acc: 0.9359\n",
      "Epoch 36/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1794 - acc: 0.9370 - val_loss: 0.1812 - val_acc: 0.9358\n",
      "Epoch 37/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1793 - acc: 0.9368 - val_loss: 0.1812 - val_acc: 0.9360\n",
      "Epoch 38/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1793 - acc: 0.9370 - val_loss: 0.1816 - val_acc: 0.9359\n",
      "Epoch 39/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1795 - acc: 0.9370 - val_loss: 0.1811 - val_acc: 0.9361\n",
      "Epoch 40/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1793 - acc: 0.9369 - val_loss: 0.1813 - val_acc: 0.9358\n",
      "Epoch 41/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1794 - acc: 0.9370 - val_loss: 0.1813 - val_acc: 0.9362\n",
      "Epoch 42/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1794 - acc: 0.9369 - val_loss: 0.1815 - val_acc: 0.9361\n",
      "Epoch 43/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1793 - acc: 0.9367 - val_loss: 0.1812 - val_acc: 0.9360\n",
      "Epoch 44/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1792 - acc: 0.9370 - val_loss: 0.1810 - val_acc: 0.9358\n",
      "Epoch 45/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1792 - acc: 0.9368 - val_loss: 0.1814 - val_acc: 0.9360\n",
      "Epoch 46/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1796 - acc: 0.9368 - val_loss: 0.1810 - val_acc: 0.9359\n",
      "Epoch 47/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1792 - acc: 0.9370 - val_loss: 0.1809 - val_acc: 0.9360\n",
      "Epoch 48/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1793 - acc: 0.9371 - val_loss: 0.1826 - val_acc: 0.9356\n",
      "Epoch 49/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1792 - acc: 0.9369 - val_loss: 0.1809 - val_acc: 0.9359\n",
      "Epoch 50/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1810 - val_acc: 0.9361\n",
      "Epoch 51/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1792 - acc: 0.9371 - val_loss: 0.1808 - val_acc: 0.9361\n",
      "Epoch 52/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1791 - acc: 0.9371 - val_loss: 0.1810 - val_acc: 0.9357\n",
      "Epoch 53/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1790 - acc: 0.9368 - val_loss: 0.1809 - val_acc: 0.9359\n",
      "Epoch 54/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1792 - acc: 0.9370 - val_loss: 0.1810 - val_acc: 0.9361\n",
      "Epoch 55/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1789 - acc: 0.9371 - val_loss: 0.1807 - val_acc: 0.9360\n",
      "Epoch 56/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1789 - acc: 0.9369 - val_loss: 0.1809 - val_acc: 0.9359\n",
      "Epoch 57/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1790 - acc: 0.9370 - val_loss: 0.1829 - val_acc: 0.9358\n",
      "Epoch 58/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1791 - acc: 0.9370 - val_loss: 0.1807 - val_acc: 0.9358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1788 - acc: 0.9371 - val_loss: 0.1807 - val_acc: 0.9360\n",
      "Epoch 60/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1789 - acc: 0.9370 - val_loss: 0.1809 - val_acc: 0.9360\n",
      "Epoch 61/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1788 - acc: 0.9369 - val_loss: 0.1811 - val_acc: 0.9359\n",
      "Epoch 62/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1788 - acc: 0.9368 - val_loss: 0.1808 - val_acc: 0.9362\n",
      "Epoch 63/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1789 - acc: 0.9369 - val_loss: 0.1808 - val_acc: 0.9359\n",
      "Epoch 64/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1788 - acc: 0.9371 - val_loss: 0.1810 - val_acc: 0.9360\n",
      "Epoch 65/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1789 - acc: 0.9370 - val_loss: 0.1816 - val_acc: 0.9362\n",
      "Epoch 66/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1791 - acc: 0.9369 - val_loss: 0.1806 - val_acc: 0.9362\n",
      "Epoch 67/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1787 - acc: 0.9369 - val_loss: 0.1810 - val_acc: 0.9360\n",
      "Epoch 68/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1791 - acc: 0.9371 - val_loss: 0.1823 - val_acc: 0.9360\n",
      "Epoch 69/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1788 - acc: 0.9369 - val_loss: 0.1805 - val_acc: 0.9360\n",
      "Epoch 70/200\n",
      "119999/119999 [==============================] - 8s 63us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1806 - val_acc: 0.9360\n",
      "Epoch 71/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1786 - acc: 0.9370 - val_loss: 0.1805 - val_acc: 0.9360\n",
      "Epoch 72/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1807 - val_acc: 0.9360\n",
      "Epoch 73/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1786 - acc: 0.9370 - val_loss: 0.1805 - val_acc: 0.9360\n",
      "Epoch 74/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1786 - acc: 0.9369 - val_loss: 0.1806 - val_acc: 0.9361\n",
      "Epoch 75/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1786 - acc: 0.9372 - val_loss: 0.1804 - val_acc: 0.9362\n",
      "Epoch 76/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1785 - acc: 0.9370 - val_loss: 0.1806 - val_acc: 0.9361\n",
      "Epoch 77/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1785 - acc: 0.9371 - val_loss: 0.1804 - val_acc: 0.9361\n",
      "Epoch 78/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1786 - acc: 0.9372 - val_loss: 0.1804 - val_acc: 0.9362\n",
      "Epoch 79/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1785 - acc: 0.9371 - val_loss: 0.1805 - val_acc: 0.9363\n",
      "Epoch 80/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1786 - acc: 0.9369 - val_loss: 0.1804 - val_acc: 0.9362\n",
      "Epoch 81/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1785 - acc: 0.9371 - val_loss: 0.1804 - val_acc: 0.9362\n",
      "Epoch 82/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1788 - acc: 0.9371 - val_loss: 0.1803 - val_acc: 0.9360\n",
      "Epoch 83/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1793 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9361\n",
      "Epoch 84/200\n",
      "119999/119999 [==============================] - 6s 53us/step - loss: 0.1787 - acc: 0.9371 - val_loss: 0.1806 - val_acc: 0.9363\n",
      "Epoch 85/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9364\n",
      "Epoch 86/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1784 - acc: 0.9371 - val_loss: 0.1803 - val_acc: 0.9364\n",
      "Epoch 87/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9364\n",
      "Epoch 88/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1783 - acc: 0.9371 - val_loss: 0.1805 - val_acc: 0.9363\n",
      "Epoch 89/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1804 - val_acc: 0.9362\n",
      "Epoch 90/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1803 - val_acc: 0.9363\n",
      "Epoch 91/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1784 - acc: 0.9370 - val_loss: 0.1803 - val_acc: 0.9363\n",
      "Epoch 92/200\n",
      "119999/119999 [==============================] - 7s 62us/step - loss: 0.1785 - acc: 0.9372 - val_loss: 0.1807 - val_acc: 0.9362\n",
      "Epoch 93/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.1803 - val_acc: 0.9365\n",
      "Epoch 94/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1787 - acc: 0.9370 - val_loss: 0.1803 - val_acc: 0.9363\n",
      "Epoch 95/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9365\n",
      "Epoch 96/200\n",
      "119999/119999 [==============================] - 8s 64us/step - loss: 0.1781 - acc: 0.9373 - val_loss: 0.1804 - val_acc: 0.9363\n",
      "Epoch 97/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1782 - acc: 0.9371 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 98/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1781 - acc: 0.9372 - val_loss: 0.1801 - val_acc: 0.9367\n",
      "Epoch 99/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1782 - acc: 0.9372 - val_loss: 0.1801 - val_acc: 0.9364\n",
      "Epoch 100/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1782 - acc: 0.9372 - val_loss: 0.1802 - val_acc: 0.9365\n",
      "Epoch 101/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.1805 - val_acc: 0.9364\n",
      "Epoch 102/200\n",
      "119999/119999 [==============================] - 8s 70us/step - loss: 0.1787 - acc: 0.9372 - val_loss: 0.1801 - val_acc: 0.9367\n",
      "Epoch 103/200\n",
      "119999/119999 [==============================] - 8s 66us/step - loss: 0.1781 - acc: 0.9372 - val_loss: 0.1807 - val_acc: 0.9354\n",
      "Epoch 104/200\n",
      "119999/119999 [==============================] - 8s 67us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1810 - val_acc: 0.9364\n",
      "Epoch 105/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1783 - acc: 0.9373 - val_loss: 0.1802 - val_acc: 0.9367\n",
      "Epoch 106/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1781 - acc: 0.9374 - val_loss: 0.1802 - val_acc: 0.9368\n",
      "Epoch 107/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1782 - acc: 0.9373 - val_loss: 0.1801 - val_acc: 0.9364\n",
      "Epoch 108/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1781 - acc: 0.9374 - val_loss: 0.1808 - val_acc: 0.9361\n",
      "Epoch 109/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1788 - acc: 0.9370 - val_loss: 0.1805 - val_acc: 0.9356\n",
      "Epoch 110/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1783 - acc: 0.9370 - val_loss: 0.1807 - val_acc: 0.9367\n",
      "Epoch 111/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1787 - acc: 0.9371 - val_loss: 0.1802 - val_acc: 0.9365\n",
      "Epoch 112/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1781 - acc: 0.9375 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 113/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1781 - acc: 0.9374 - val_loss: 0.1824 - val_acc: 0.9365\n",
      "Epoch 114/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1786 - acc: 0.9371 - val_loss: 0.1800 - val_acc: 0.9365\n",
      "Epoch 115/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1785 - acc: 0.9375 - val_loss: 0.1800 - val_acc: 0.9368\n",
      "Epoch 116/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1780 - acc: 0.9372 - val_loss: 0.1799 - val_acc: 0.9368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1779 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9367\n",
      "Epoch 118/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 119/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1779 - acc: 0.9373 - val_loss: 0.1803 - val_acc: 0.9366\n",
      "Epoch 120/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1783 - acc: 0.9373 - val_loss: 0.1799 - val_acc: 0.9370\n",
      "Epoch 121/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1783 - acc: 0.9373 - val_loss: 0.1807 - val_acc: 0.9362\n",
      "Epoch 122/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1783 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 123/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1778 - acc: 0.9375 - val_loss: 0.1799 - val_acc: 0.9368\n",
      "Epoch 124/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 125/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1778 - acc: 0.9373 - val_loss: 0.1799 - val_acc: 0.9367\n",
      "Epoch 126/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1778 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 127/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1779 - acc: 0.9372 - val_loss: 0.1800 - val_acc: 0.9367\n",
      "Epoch 128/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1779 - acc: 0.9373 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 129/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1779 - acc: 0.9375 - val_loss: 0.1800 - val_acc: 0.9362\n",
      "Epoch 130/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1778 - acc: 0.9375 - val_loss: 0.1804 - val_acc: 0.9369\n",
      "Epoch 131/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1778 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9368\n",
      "Epoch 132/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1801 - val_acc: 0.9365\n",
      "Epoch 133/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.1804 - val_acc: 0.9367\n",
      "Epoch 134/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.1800 - val_acc: 0.9366\n",
      "Epoch 135/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1777 - acc: 0.9371 - val_loss: 0.1802 - val_acc: 0.9368\n",
      "Epoch 136/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9366\n",
      "Epoch 137/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1777 - acc: 0.9374 - val_loss: 0.1798 - val_acc: 0.9368\n",
      "Epoch 138/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1777 - acc: 0.9373 - val_loss: 0.1801 - val_acc: 0.9363\n",
      "Epoch 139/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1798 - val_acc: 0.9368\n",
      "Epoch 140/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 141/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1800 - val_acc: 0.9367\n",
      "Epoch 142/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1777 - acc: 0.9374 - val_loss: 0.1801 - val_acc: 0.9369\n",
      "Epoch 143/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1779 - acc: 0.9376 - val_loss: 0.1799 - val_acc: 0.9361\n",
      "Epoch 144/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9368\n",
      "Epoch 145/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9365\n",
      "Epoch 146/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1777 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9363\n",
      "Epoch 147/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9368\n",
      "Epoch 148/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 149/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9373 - val_loss: 0.1799 - val_acc: 0.9367\n",
      "Epoch 150/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 151/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 152/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9366\n",
      "Epoch 153/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 154/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 155/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9366\n",
      "Epoch 156/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1778 - acc: 0.9376 - val_loss: 0.1808 - val_acc: 0.9362\n",
      "Epoch 157/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 158/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1805 - val_acc: 0.9359\n",
      "Epoch 159/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1774 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 160/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1775 - acc: 0.9373 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 161/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1778 - acc: 0.9376 - val_loss: 0.1804 - val_acc: 0.9364\n",
      "Epoch 162/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 163/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1774 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9362\n",
      "Epoch 164/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9366\n",
      "Epoch 165/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1773 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9363\n",
      "Epoch 166/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1774 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9364\n",
      "Epoch 167/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1774 - acc: 0.9377 - val_loss: 0.1801 - val_acc: 0.9367\n",
      "Epoch 168/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1776 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9366\n",
      "Epoch 169/200\n",
      "119999/119999 [==============================] - 6s 54us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9363\n",
      "Epoch 170/200\n",
      "119999/119999 [==============================] - 7s 55us/step - loss: 0.1773 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9367\n",
      "Epoch 171/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1774 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 172/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1778 - acc: 0.9376 - val_loss: 0.1808 - val_acc: 0.9365\n",
      "Epoch 173/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1775 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9364\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1773 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9365\n",
      "Epoch 175/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1775 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 176/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1772 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9363\n",
      "Epoch 177/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9366\n",
      "Epoch 178/200\n",
      "119999/119999 [==============================] - 7s 56us/step - loss: 0.1775 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9364\n",
      "Epoch 179/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1773 - acc: 0.9376 - val_loss: 0.1797 - val_acc: 0.9368\n",
      "Epoch 180/200\n",
      "119999/119999 [==============================] - 7s 54us/step - loss: 0.1774 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9362\n",
      "Epoch 181/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1773 - acc: 0.9374 - val_loss: 0.1797 - val_acc: 0.9364\n",
      "Epoch 182/200\n",
      "119999/119999 [==============================] - 9s 72us/step - loss: 0.1777 - acc: 0.9376 - val_loss: 0.1806 - val_acc: 0.9368\n",
      "Epoch 183/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1774 - acc: 0.9376 - val_loss: 0.1798 - val_acc: 0.9364\n",
      "Epoch 184/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1774 - acc: 0.9377 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 185/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1773 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9364\n",
      "Epoch 186/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1773 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 187/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1772 - acc: 0.9376 - val_loss: 0.1797 - val_acc: 0.9363\n",
      "Epoch 188/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1771 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9365\n",
      "Epoch 189/200\n",
      "119999/119999 [==============================] - 7s 61us/step - loss: 0.1774 - acc: 0.9374 - val_loss: 0.1798 - val_acc: 0.9367\n",
      "Epoch 190/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1773 - acc: 0.9374 - val_loss: 0.1799 - val_acc: 0.9363\n",
      "Epoch 191/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1798 - val_acc: 0.9362\n",
      "Epoch 192/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1771 - acc: 0.9377 - val_loss: 0.1795 - val_acc: 0.9363\n",
      "Epoch 193/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1771 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9361\n",
      "Epoch 194/200\n",
      "119999/119999 [==============================] - 7s 60us/step - loss: 0.1771 - acc: 0.9374 - val_loss: 0.1795 - val_acc: 0.9362\n",
      "Epoch 195/200\n",
      "119999/119999 [==============================] - 7s 57us/step - loss: 0.1771 - acc: 0.9377 - val_loss: 0.1798 - val_acc: 0.9363\n",
      "Epoch 196/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1772 - acc: 0.9376 - val_loss: 0.1796 - val_acc: 0.9363\n",
      "Epoch 197/200\n",
      "119999/119999 [==============================] - 7s 59us/step - loss: 0.1771 - acc: 0.9375 - val_loss: 0.1797 - val_acc: 0.9366\n",
      "Epoch 198/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1772 - acc: 0.9374 - val_loss: 0.1796 - val_acc: 0.9367\n",
      "Epoch 199/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1774 - acc: 0.9375 - val_loss: 0.1796 - val_acc: 0.9360\n",
      "Epoch 200/200\n",
      "119999/119999 [==============================] - 7s 58us/step - loss: 0.1772 - acc: 0.9377 - val_loss: 0.1802 - val_acc: 0.9361\n"
     ]
    }
   ],
   "source": [
    "Final_model = Sequential()\n",
    "Final_model.add(Dense(units = 600, input_dim = 10, activation = 'sigmoid'))\n",
    "Final_model.add(Dense(units = 300, activation = 'sigmoid'))\n",
    "Final_model.add(Dense(units = 100, activation = 'sigmoid'))\n",
    "Final_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "Final_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "Final_log = Final_model.fit(X_train, Y_train, batch_size= 6000, epochs=200, verbose=1,\n",
    "                validation_data=(X_validation, Y_validation))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Final_Deep_neural_network_predict = Final_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27659   310]\n",
      " [ 1608   423]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_validation, [np.round(pred) for pred in Final_Deep_neural_network_predict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = pd.DataFrame(Final_model.predict(X_test))\n",
    "Y_predict['Id'] = range(1, len(Y_predict) + 1)\n",
    "Y_predict['probability'] = Y_predict.iloc[:,[0]]\n",
    "Y_predict = Y_predict.iloc[:,1:]\n",
    "Y_predict.to_csv('Result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.073911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.079752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.013617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.100619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.125391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.030466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.047796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.038894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.495168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.014154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.013012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.014163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.069329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.058021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.017452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.033220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.018046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.254618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.100238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.011319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.013023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.004463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.528736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.105695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.015738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.127361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.011397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.074468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.022972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101473</th>\n",
       "      <td>101474</td>\n",
       "      <td>0.052650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101474</th>\n",
       "      <td>101475</td>\n",
       "      <td>0.313572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101475</th>\n",
       "      <td>101476</td>\n",
       "      <td>0.185529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101476</th>\n",
       "      <td>101477</td>\n",
       "      <td>0.008884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101477</th>\n",
       "      <td>101478</td>\n",
       "      <td>0.007056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101478</th>\n",
       "      <td>101479</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101479</th>\n",
       "      <td>101480</td>\n",
       "      <td>0.137739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101480</th>\n",
       "      <td>101481</td>\n",
       "      <td>0.032968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101481</th>\n",
       "      <td>101482</td>\n",
       "      <td>0.016608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101482</th>\n",
       "      <td>101483</td>\n",
       "      <td>0.511068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101483</th>\n",
       "      <td>101484</td>\n",
       "      <td>0.004757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101484</th>\n",
       "      <td>101485</td>\n",
       "      <td>0.018033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101485</th>\n",
       "      <td>101486</td>\n",
       "      <td>0.004250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101486</th>\n",
       "      <td>101487</td>\n",
       "      <td>0.028655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101487</th>\n",
       "      <td>101488</td>\n",
       "      <td>0.007236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101488</th>\n",
       "      <td>101489</td>\n",
       "      <td>0.009670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101489</th>\n",
       "      <td>101490</td>\n",
       "      <td>0.134934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101490</th>\n",
       "      <td>101491</td>\n",
       "      <td>0.383161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101491</th>\n",
       "      <td>101492</td>\n",
       "      <td>0.061180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101492</th>\n",
       "      <td>101493</td>\n",
       "      <td>0.075681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101493</th>\n",
       "      <td>101494</td>\n",
       "      <td>0.007680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101494</th>\n",
       "      <td>101495</td>\n",
       "      <td>0.014747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101495</th>\n",
       "      <td>101496</td>\n",
       "      <td>0.334708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101496</th>\n",
       "      <td>101497</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101497</th>\n",
       "      <td>101498</td>\n",
       "      <td>0.005954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101498</th>\n",
       "      <td>101499</td>\n",
       "      <td>0.035251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101499</th>\n",
       "      <td>101500</td>\n",
       "      <td>0.388013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101500</th>\n",
       "      <td>101501</td>\n",
       "      <td>0.004921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101501</th>\n",
       "      <td>101502</td>\n",
       "      <td>0.114865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101502</th>\n",
       "      <td>101503</td>\n",
       "      <td>0.063722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101503 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  probability\n",
       "0            1     0.073911\n",
       "1            2     0.079752\n",
       "2            3     0.013617\n",
       "3            4     0.100619\n",
       "4            5     0.125391\n",
       "5            6     0.030466\n",
       "6            7     0.047796\n",
       "7            8     0.038894\n",
       "8            9     0.002612\n",
       "9           10     0.495168\n",
       "10          11     0.014154\n",
       "11          12     0.013012\n",
       "12          13     0.014163\n",
       "13          14     0.069329\n",
       "14          15     0.058021\n",
       "15          16     0.017452\n",
       "16          17     0.033220\n",
       "17          18     0.018046\n",
       "18          19     0.254618\n",
       "19          20     0.100238\n",
       "20          21     0.011319\n",
       "21          22     0.013023\n",
       "22          23     0.004463\n",
       "23          24     0.528736\n",
       "24          25     0.105695\n",
       "25          26     0.015738\n",
       "26          27     0.127361\n",
       "27          28     0.011397\n",
       "28          29     0.074468\n",
       "29          30     0.022972\n",
       "...        ...          ...\n",
       "101473  101474     0.052650\n",
       "101474  101475     0.313572\n",
       "101475  101476     0.185529\n",
       "101476  101477     0.008884\n",
       "101477  101478     0.007056\n",
       "101478  101479     0.010189\n",
       "101479  101480     0.137739\n",
       "101480  101481     0.032968\n",
       "101481  101482     0.016608\n",
       "101482  101483     0.511068\n",
       "101483  101484     0.004757\n",
       "101484  101485     0.018033\n",
       "101485  101486     0.004250\n",
       "101486  101487     0.028655\n",
       "101487  101488     0.007236\n",
       "101488  101489     0.009670\n",
       "101489  101490     0.134934\n",
       "101490  101491     0.383161\n",
       "101491  101492     0.061180\n",
       "101492  101493     0.075681\n",
       "101493  101494     0.007680\n",
       "101494  101495     0.014747\n",
       "101495  101496     0.334708\n",
       "101496  101497     0.007400\n",
       "101497  101498     0.005954\n",
       "101498  101499     0.035251\n",
       "101499  101500     0.388013\n",
       "101500  101501     0.004921\n",
       "101501  101502     0.114865\n",
       "101502  101503     0.063722\n",
       "\n",
       "[101503 rows x 2 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dian\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier = RandomForestClassifier(random_state=42,n_estimators=300, max_depth=5, class_weight=\"balanced\")\n",
    "RandomForestClassifier.fit(X_train, Y_train)\n",
    "random_forest_predict = RandomForestClassifier.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.78      0.87     27969\n",
      "          1       0.20      0.78      0.32      2031\n",
      "\n",
      "avg / total       0.93      0.78      0.83     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_validation, random_forest_predict, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21753  6216]\n",
      " [  440  1591]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_validation, random_forest_predict))\n",
    "del random_forest_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
